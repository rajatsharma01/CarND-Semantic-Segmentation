np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
TensorFlow Version: 1.4.0
2018-09-05 23:10:07.012977: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
main.py:18: UserWarning: No GPU found. Please use a GPU to train your neural network.
  warnings.warn('No GPU found. Please use a GPU to train your neural network.')
Tests Passed
Tests Passed
Tests Passed
Tests Passed
Tests Passed
Starting Epoch: 1, at: 1536214220.3403206
=========================================
Batch: 1. Loss: 55.23427963256836
Batch: 2. Loss: 259.957763671875
Batch: 3. Loss: 22.510883331298828
Batch: 4. Loss: 22.525188446044922
Batch: 5. Loss: 15.48765754699707
Batch: 6. Loss: 9.417146682739258
Batch: 7. Loss: 5.1573004722595215
Batch: 8. Loss: 5.638235569000244
Batch: 9. Loss: 2.9089229106903076
Batch: 10. Loss: 2.1305582523345947
Batch: 11. Loss: 2.207101821899414
Batch: 12. Loss: 1.9747645854949951
Batch: 13. Loss: 1.6282050609588623
Batch: 14. Loss: 0.9099781513214111
Batch: 15. Loss: 1.1942164897918701
-----------------------------------------
Epoch completed in 1244.903047323227 s.
Mean loss: 27.258813858032227

Starting Epoch: 2, at: 1536215465.2433975
=========================================
Batch: 1. Loss: 1.1638273000717163
Batch: 2. Loss: 0.9120489954948425
Batch: 3. Loss: 0.7809600234031677
Batch: 4. Loss: 0.7831701636314392
Batch: 5. Loss: 0.7482480406761169
Batch: 6. Loss: 0.7422012090682983
Batch: 7. Loss: 0.7597874402999878
Batch: 8. Loss: 0.7432358264923096
Batch: 9. Loss: 0.7272098660469055
Batch: 10. Loss: 0.7052212357521057
Batch: 11. Loss: 0.7043489813804626
Batch: 12. Loss: 0.7030166387557983
Batch: 13. Loss: 0.6977707147598267
Batch: 14. Loss: 0.6984431743621826
Batch: 15. Loss: 0.6916524767875671
-----------------------------------------
Epoch completed in 1228.8207049369812 s.
Mean loss: 0.7707427740097046

Starting Epoch: 3, at: 1536216694.0642555
=========================================
Batch: 1. Loss: 0.6918994784355164
Batch: 2. Loss: 0.6908008456230164
Batch: 3. Loss: 0.6852281093597412
Batch: 4. Loss: 0.6805905699729919
Batch: 5. Loss: 0.6786542534828186
Batch: 6. Loss: 0.6758074164390564
Batch: 7. Loss: 0.6686385273933411
Batch: 8. Loss: 0.6716417074203491
Batch: 9. Loss: 0.6714901328086853
Batch: 10. Loss: 0.6698766946792603
Batch: 11. Loss: 0.6672029495239258
Batch: 12. Loss: 0.6675933003425598
Batch: 13. Loss: 0.6630669236183167
Batch: 14. Loss: 0.6603924036026001
Batch: 15. Loss: 0.6602712273597717
-----------------------------------------
Epoch completed in 1209.0557782649994 s.
Mean loss: 0.6735436916351318

Starting Epoch: 4, at: 1536217903.1200638
=========================================
Batch: 1. Loss: 0.657221794128418
Batch: 2. Loss: 0.661577582359314
Batch: 3. Loss: 0.6598254442214966
Batch: 4. Loss: 0.6596812605857849
Batch: 5. Loss: 0.6563305854797363
Batch: 6. Loss: 0.6536709666252136
Batch: 7. Loss: 0.6545761227607727
Batch: 8. Loss: 0.651490330696106
Batch: 9. Loss: 0.6531702876091003
Batch: 10. Loss: 0.6523122787475586
Batch: 11. Loss: 0.6466780304908752
Batch: 12. Loss: 0.654391348361969
Batch: 13. Loss: 0.6467813849449158
Batch: 14. Loss: 0.6493138074874878
Batch: 15. Loss: 0.648619532585144
-----------------------------------------
Epoch completed in 1211.3090660572052 s.
Mean loss: 0.6537094116210938

Starting Epoch: 5, at: 1536219114.4294412
=========================================
Batch: 1. Loss: 0.6471275687217712
Batch: 2. Loss: 0.643749475479126
Batch: 3. Loss: 0.6435892581939697
Batch: 4. Loss: 0.6458935737609863
Batch: 5. Loss: 0.6458215117454529
Batch: 6. Loss: 0.6378809213638306
Batch: 7. Loss: 0.6401439905166626
Batch: 8. Loss: 0.6400317549705505
Batch: 9. Loss: 0.6360154747962952
Batch: 10. Loss: 0.6426421403884888
Batch: 11. Loss: 0.6390464901924133
Batch: 12. Loss: 0.637292742729187
Batch: 13. Loss: 0.6410674452781677
Batch: 14. Loss: 0.6349576115608215
Batch: 15. Loss: 0.6314870119094849
-----------------------------------------
Epoch completed in 1181.5331444740295 s.
Mean loss: 0.6404497623443604

Starting Epoch: 6, at: 1536220295.9626243
=========================================
Batch: 1. Loss: 0.6338688135147095
Batch: 2. Loss: 0.6297816038131714
Batch: 3. Loss: 0.6282397508621216
Batch: 4. Loss: 0.6343916654586792
Batch: 5. Loss: 0.6370304226875305
Batch: 6. Loss: 0.631500780582428
Batch: 7. Loss: 0.6308314800262451
Batch: 8. Loss: 0.6281630992889404
Batch: 9. Loss: 0.6256897449493408
Batch: 10. Loss: 0.633877158164978
Batch: 11. Loss: 0.625457763671875
Batch: 12. Loss: 0.6296325922012329
Batch: 13. Loss: 0.6217616200447083
Batch: 14. Loss: 0.6279704570770264
Batch: 15. Loss: 0.6270079612731934
-----------------------------------------
Epoch completed in 1175.039218902588 s.
Mean loss: 0.6296803951263428

Starting Epoch: 7, at: 1536221471.0018733
=========================================
Batch: 1. Loss: 0.6182408928871155
Batch: 2. Loss: 0.6234919428825378
Batch: 3. Loss: 0.6240025758743286
Batch: 4. Loss: 0.6253060102462769
Batch: 5. Loss: 0.6240730881690979
Batch: 6. Loss: 0.6192024946212769
Batch: 7. Loss: 0.6231053471565247
Batch: 8. Loss: 0.6178671717643738
Batch: 9. Loss: 0.6187289953231812
Batch: 10. Loss: 0.6233285665512085
Batch: 11. Loss: 0.6139678359031677
Batch: 12. Loss: 0.6193798780441284
Batch: 13. Loss: 0.6133841872215271
Batch: 14. Loss: 0.6160370111465454
Batch: 15. Loss: 0.6074140667915344
-----------------------------------------
Epoch completed in 1173.8806548118591 s.
Mean loss: 0.6191686987876892

Starting Epoch: 8, at: 1536222644.8825567
=========================================
Batch: 1. Loss: 0.6128301620483398
Batch: 2. Loss: 0.612277626991272
Batch: 3. Loss: 0.6093621850013733
Batch: 4. Loss: 0.6085195541381836
Batch: 5. Loss: 0.6091158986091614
Batch: 6. Loss: 0.6084265112876892
Batch: 7. Loss: 0.6134088039398193
Batch: 8. Loss: 0.6138541102409363
Batch: 9. Loss: 0.607545018196106
Batch: 10. Loss: 0.6163791418075562
Batch: 11. Loss: 0.6056627631187439
Batch: 12. Loss: 0.5989010334014893
Batch: 13. Loss: 0.6037111282348633
Batch: 14. Loss: 0.6074919700622559
Batch: 15. Loss: 0.6183324456214905
-----------------------------------------
Epoch completed in 1174.5443351268768 s.
Mean loss: 0.6097211837768555

Starting Epoch: 9, at: 1536223819.4269466
=========================================
Batch: 1. Loss: 0.6086717247962952
Batch: 2. Loss: 0.6014791131019592
Batch: 3. Loss: 0.6058757901191711
Batch: 4. Loss: 0.5985127687454224
Batch: 5. Loss: 0.5991292595863342
Batch: 6. Loss: 0.6003216505050659
Batch: 7. Loss: 0.6029123067855835
Batch: 8. Loss: 0.601858913898468
Batch: 9. Loss: 0.6027959585189819
Batch: 10. Loss: 0.5942899584770203
Batch: 11. Loss: 0.5977986454963684
Batch: 12. Loss: 0.5925489664077759
Batch: 13. Loss: 0.5918130874633789
Batch: 14. Loss: 0.5951787829399109
Batch: 15. Loss: 0.594834566116333
-----------------------------------------
Epoch completed in 1177.0872864723206 s.
Mean loss: 0.5992014408111572

Starting Epoch: 10, at: 1536224996.5144544
=========================================
Batch: 1. Loss: 0.5925830006599426
Batch: 2. Loss: 0.5894641280174255
Batch: 3. Loss: 0.5918587446212769
Batch: 4. Loss: 0.5891589522361755
Batch: 5. Loss: 0.5871005654335022
Batch: 6. Loss: 0.5875547528266907
Batch: 7. Loss: 0.5977256894111633
Batch: 8. Loss: 0.5859876871109009
Batch: 9. Loss: 0.5933260321617126
Batch: 10. Loss: 0.5871167778968811
Batch: 11. Loss: 0.5862703323364258
Batch: 12. Loss: 0.5819743275642395
Batch: 13. Loss: 0.5854725241661072
Batch: 14. Loss: 0.5850673317909241
Batch: 15. Loss: 0.5830402970314026
-----------------------------------------
Epoch completed in 1171.7422139644623 s.
Mean loss: 0.588246762752533

Starting Epoch: 11, at: 1536226168.2568913
=========================================
Batch: 1. Loss: 0.5804660320281982
Batch: 2. Loss: 0.5840885639190674
Batch: 3. Loss: 0.5781599879264832
Batch: 4. Loss: 0.5810418128967285
Batch: 5. Loss: 0.5771862864494324
Batch: 6. Loss: 0.578490138053894
Batch: 7. Loss: 0.5801995396614075
Batch: 8. Loss: 0.5754128098487854
Batch: 9. Loss: 0.5701324939727783
Batch: 10. Loss: 0.5832623839378357
Batch: 11. Loss: 0.5746514201164246
Batch: 12. Loss: 0.580323338508606
Batch: 13. Loss: 0.5707479119300842
Batch: 14. Loss: 0.5695621967315674
Batch: 15. Loss: 0.5707427859306335
-----------------------------------------
Epoch completed in 1172.2542796134949 s.
Mean loss: 0.5769644379615784

Starting Epoch: 12, at: 1536227340.5112092
=========================================
Batch: 1. Loss: 0.5629868507385254
Batch: 2. Loss: 0.5722317099571228
Batch: 3. Loss: 0.5618461966514587
Batch: 4. Loss: 0.5733842849731445
Batch: 5. Loss: 0.5683265924453735
Batch: 6. Loss: 0.5711178779602051
Batch: 7. Loss: 0.5687074065208435
Batch: 8. Loss: 0.5626628994941711
Batch: 9. Loss: 0.564450740814209
Batch: 10. Loss: 0.5633288025856018
Batch: 11. Loss: 0.5598795413970947
Batch: 12. Loss: 0.5672124624252319
Batch: 13. Loss: 0.5604730844497681
Batch: 14. Loss: 0.5579537749290466
Batch: 15. Loss: 0.557193398475647
-----------------------------------------
Epoch completed in 1175.3650431632996 s.
Mean loss: 0.5647837519645691

Starting Epoch: 13, at: 1536228515.8764725
=========================================
Batch: 1. Loss: 0.5505658984184265
Batch: 2. Loss: 0.5563971400260925
Batch: 3. Loss: 0.5594989061355591
Batch: 4. Loss: 0.5517706871032715
Batch: 5. Loss: 0.5488646030426025
Batch: 6. Loss: 0.5544763803482056
Batch: 7. Loss: 0.5523686408996582
Batch: 8. Loss: 0.5512698888778687
Batch: 9. Loss: 0.5528600215911865
Batch: 10. Loss: 0.553096354007721
Batch: 11. Loss: 0.5551772713661194
Batch: 12. Loss: 0.5470731854438782
Batch: 13. Loss: 0.5407413244247437
Batch: 14. Loss: 0.5489206314086914
Batch: 15. Loss: 0.553543746471405
-----------------------------------------
Epoch completed in 1171.9801530838013 s.
Mean loss: 0.5517749786376953

Starting Epoch: 14, at: 1536229687.8566618
=========================================
Batch: 1. Loss: 0.5424374938011169
Batch: 2. Loss: 0.5520539879798889
Batch: 3. Loss: 0.5302321910858154
Batch: 4. Loss: 0.5342752933502197
Batch: 5. Loss: 0.5370065569877625
Batch: 6. Loss: 0.5354381799697876
Batch: 7. Loss: 0.5327544808387756
Batch: 8. Loss: 0.5293704867362976
Batch: 9. Loss: 0.5385805368423462
Batch: 10. Loss: 0.535335123538971
Batch: 11. Loss: 0.5479691028594971
Batch: 12. Loss: 0.5484116077423096
Batch: 13. Loss: 0.5275500416755676
Batch: 14. Loss: 0.5353796482086182
Batch: 15. Loss: 0.5237843990325928
-----------------------------------------
Epoch completed in 1174.2708110809326 s.
Mean loss: 0.5367052555084229

Starting Epoch: 15, at: 1536230862.1275103
=========================================
Batch: 1. Loss: 0.5412054657936096
Batch: 2. Loss: 0.533733606338501
Batch: 3. Loss: 0.5341931581497192
Batch: 4. Loss: 0.5252733826637268
Batch: 5. Loss: 0.513564944267273
Batch: 6. Loss: 0.5062428116798401
Batch: 7. Loss: 0.5297236442565918
Batch: 8. Loss: 0.5191724896430969
Batch: 9. Loss: 0.525814414024353
Batch: 10. Loss: 0.5106649398803711
Batch: 11. Loss: 0.5234284996986389
Batch: 12. Loss: 0.5122833847999573
Batch: 13. Loss: 0.5127366781234741
Batch: 14. Loss: 0.5061087608337402
Batch: 15. Loss: 0.5025352835655212
-----------------------------------------
Epoch completed in 1175.451514005661 s.
Mean loss: 0.5197787880897522

Starting Epoch: 16, at: 1536232037.5790539
=========================================
Batch: 1. Loss: 0.5175610184669495
Batch: 2. Loss: 0.49521541595458984
Batch: 3. Loss: 0.5036701560020447
Batch: 4. Loss: 0.5047206878662109
Batch: 5. Loss: 0.5022082328796387
Batch: 6. Loss: 0.5018817186355591
Batch: 7. Loss: 0.4952080249786377
Batch: 8. Loss: 0.4992808699607849
Batch: 9. Loss: 0.5072281360626221
Batch: 10. Loss: 0.5019263625144958
Batch: 11. Loss: 0.5041685104370117
Batch: 12. Loss: 0.5000748634338379
Batch: 13. Loss: 0.4922460913658142
Batch: 14. Loss: 0.5000735521316528
Batch: 15. Loss: 0.4668995141983032
-----------------------------------------
Epoch completed in 1170.5889067649841 s.
Mean loss: 0.4994908273220062

Starting Epoch: 17, at: 1536233208.167987
=========================================
Batch: 1. Loss: 0.482827365398407
Batch: 2. Loss: 0.4986737072467804
Batch: 3. Loss: 0.4807755947113037
Batch: 4. Loss: 0.4768810272216797
Batch: 5. Loss: 0.4843859076499939
Batch: 6. Loss: 0.48305362462997437
Batch: 7. Loss: 0.47072964906692505
Batch: 8. Loss: 0.4742099344730377
Batch: 9. Loss: 0.47005531191825867
Batch: 10. Loss: 0.47014346718788147
Batch: 11. Loss: 0.4670591354370117
Batch: 12. Loss: 0.46038007736206055
Batch: 13. Loss: 0.4657362997531891
Batch: 14. Loss: 0.45561474561691284
Batch: 15. Loss: 0.45386070013046265
-----------------------------------------
Epoch completed in 1172.90389585495 s.
Mean loss: 0.47295907139778137

Starting Epoch: 18, at: 1536234381.0719151
=========================================
Batch: 1. Loss: 0.45531588792800903
Batch: 2. Loss: 0.46298885345458984
Batch: 3. Loss: 0.45755475759506226
Batch: 4. Loss: 0.452438086271286
Batch: 5. Loss: 0.44060853123664856
Batch: 6. Loss: 0.44791722297668457
Batch: 7. Loss: 0.4421359598636627
Batch: 8. Loss: 0.4456048607826233
Batch: 9. Loss: 0.4448001980781555
Batch: 10. Loss: 0.43079277873039246
Batch: 11. Loss: 0.4172135293483734
Batch: 12. Loss: 0.44814300537109375
Batch: 13. Loss: 0.4260437786579132
Batch: 14. Loss: 0.4263526499271393
Batch: 15. Loss: 0.4151548147201538
-----------------------------------------
Epoch completed in 1176.0755031108856 s.
Mean loss: 0.4408710300922394

Starting Epoch: 19, at: 1536235557.1474495
=========================================
Batch: 1. Loss: 0.4171343445777893
Batch: 2. Loss: 0.41251593828201294
Batch: 3. Loss: 0.4100678265094757
Batch: 4. Loss: 0.41515520215034485
Batch: 5. Loss: 0.4044935405254364
Batch: 6. Loss: 0.4193338453769684
Batch: 7. Loss: 0.40681329369544983
Batch: 8. Loss: 0.4035312533378601
Batch: 9. Loss: 0.3944692015647888
Batch: 10. Loss: 0.39275193214416504
Batch: 11. Loss: 0.3934677541255951
Batch: 12. Loss: 0.3979463577270508
Batch: 13. Loss: 0.3915548324584961
Batch: 14. Loss: 0.37397301197052
Batch: 15. Loss: 0.3534035086631775
-----------------------------------------
Epoch completed in 1171.872151851654 s.
Mean loss: 0.3991074562072754

Starting Epoch: 20, at: 1536236729.0196283
=========================================
Batch: 1. Loss: 0.37639325857162476
Batch: 2. Loss: 0.3807732164859772
Batch: 3. Loss: 0.37391212582588196
Batch: 4. Loss: 0.35080093145370483
Batch: 5. Loss: 0.3621860146522522
Batch: 6. Loss: 0.37222012877464294
Batch: 7. Loss: 0.4325115382671356
Batch: 8. Loss: 0.3871805965900421
Batch: 9. Loss: 0.42317497730255127
Batch: 10. Loss: 0.36059510707855225
Batch: 11. Loss: 0.3836130201816559
Batch: 12. Loss: 0.4317531883716583
Batch: 13. Loss: 0.34693488478660583
Batch: 14. Loss: 0.41079312562942505
Batch: 15. Loss: 0.4160490036010742
-----------------------------------------
Epoch completed in 1171.944253206253 s.
Mean loss: 0.38725942373275757

Starting Epoch: 21, at: 1536237900.9639122
=========================================
Batch: 1. Loss: 0.3562670350074768
Batch: 2. Loss: 0.3598019778728485
Batch: 3. Loss: 0.39442741870880127
Batch: 4. Loss: 0.36096930503845215
Batch: 5. Loss: 0.3333318531513214
Batch: 6. Loss: 0.34621745347976685
Batch: 7. Loss: 0.36376872658729553
Batch: 8. Loss: 0.37308767437934875
Batch: 9. Loss: 0.33101895451545715
Batch: 10. Loss: 0.3448708653450012
Batch: 11. Loss: 0.32504886388778687
Batch: 12. Loss: 0.3159107267856598
Batch: 13. Loss: 0.3454376459121704
Batch: 14. Loss: 0.31755414605140686
Batch: 15. Loss: 0.31320035457611084
-----------------------------------------
Epoch completed in 1170.657508611679 s.
Mean loss: 0.34539416432380676

Starting Epoch: 22, at: 1536239071.621449
=========================================
Batch: 1. Loss: 0.3005768060684204
Batch: 2. Loss: 0.30441421270370483
Batch: 3. Loss: 0.3302176594734192
Batch: 4. Loss: 0.29850640892982483
Batch: 5. Loss: 0.2974834442138672
Batch: 6. Loss: 0.35243794322013855
Batch: 7. Loss: 0.29733261466026306
Batch: 8. Loss: 0.34776583313941956
Batch: 9. Loss: 0.2944929897785187
Batch: 10. Loss: 0.2866855263710022
Batch: 11. Loss: 0.29585719108581543
Batch: 12. Loss: 0.29493415355682373
Batch: 13. Loss: 0.28492578864097595
Batch: 14. Loss: 0.26982665061950684
Batch: 15. Loss: 0.2971135973930359
-----------------------------------------
Epoch completed in 1168.516521692276 s.
Mean loss: 0.30350467562675476

Starting Epoch: 23, at: 1536240240.1380093
=========================================
Batch: 1. Loss: 0.2735747694969177
Batch: 2. Loss: 0.2938801348209381
Batch: 3. Loss: 0.2887686491012573
Batch: 4. Loss: 0.28061673045158386
Batch: 5. Loss: 0.3223409652709961
Batch: 6. Loss: 0.28117015957832336
Batch: 7. Loss: 0.287659615278244
Batch: 8. Loss: 0.3500427305698395
Batch: 9. Loss: 0.27093929052352905
Batch: 10. Loss: 0.274600088596344
Batch: 11. Loss: 0.324026495218277
Batch: 12. Loss: 0.3074374496936798
Batch: 13. Loss: 0.28717195987701416
Batch: 14. Loss: 0.2686065137386322
Batch: 15. Loss: 0.27679356932640076
-----------------------------------------
Epoch completed in 1173.120219707489 s.
Mean loss: 0.292508602142334

Starting Epoch: 24, at: 1536241413.258265
=========================================
Batch: 1. Loss: 0.27761825919151306
Batch: 2. Loss: 0.25964751839637756
Batch: 3. Loss: 0.2561090886592865
Batch: 4. Loss: 0.27109381556510925
Batch: 5. Loss: 0.263887494802475
Batch: 6. Loss: 0.2652236521244049
Batch: 7. Loss: 0.26749518513679504
Batch: 8. Loss: 0.2632003426551819
Batch: 9. Loss: 0.29083847999572754
Batch: 10. Loss: 0.26263880729675293
Batch: 11. Loss: 0.26479488611221313
Batch: 12. Loss: 0.31603407859802246
Batch: 13. Loss: 0.2880755662918091
Batch: 14. Loss: 0.2539719045162201
Batch: 15. Loss: 0.31899166107177734
-----------------------------------------
Epoch completed in 1175.6985077857971 s.
Mean loss: 0.27464136481285095

Starting Epoch: 25, at: 1536242588.9568007
=========================================
Batch: 1. Loss: 0.25233450531959534
Batch: 2. Loss: 0.25317156314849854
Batch: 3. Loss: 0.24574591219425201
Batch: 4. Loss: 0.24552643299102783
Batch: 5. Loss: 0.24290506541728973
Batch: 6. Loss: 0.2524169385433197
Batch: 7. Loss: 0.2599491775035858
Batch: 8. Loss: 0.2837437093257904
Batch: 9. Loss: 0.2426522821187973
Batch: 10. Loss: 0.26109257340431213
Batch: 11. Loss: 0.2819161117076874
Batch: 12. Loss: 0.2571888864040375
Batch: 13. Loss: 0.2563590705394745
Batch: 14. Loss: 0.2559160590171814
Batch: 15. Loss: 0.29334554076194763
-----------------------------------------
Epoch completed in 1168.4841146469116 s.
Mean loss: 0.2589509189128876

Starting Epoch: 26, at: 1536243757.4409451
=========================================
Batch: 1. Loss: 0.25125542283058167
Batch: 2. Loss: 0.2552304267883301
Batch: 3. Loss: 0.27814510464668274
Batch: 4. Loss: 0.23320767283439636
Batch: 5. Loss: 0.25788357853889465
Batch: 6. Loss: 0.25328564643859863
Batch: 7. Loss: 0.224180668592453
Batch: 8. Loss: 0.27034491300582886
Batch: 9. Loss: 0.22157128155231476
Batch: 10. Loss: 0.25282037258148193
Batch: 11. Loss: 0.24435892701148987
Batch: 12. Loss: 0.2437412589788437
Batch: 13. Loss: 0.22462524473667145
Batch: 14. Loss: 0.22484908998012543
Batch: 15. Loss: 0.21014901995658875
-----------------------------------------
Epoch completed in 1174.20494556427 s.
Mean loss: 0.2430432289838791

Starting Epoch: 27, at: 1536244931.645927
=========================================
Batch: 1. Loss: 0.2522869110107422
Batch: 2. Loss: 0.2140394151210785
Batch: 3. Loss: 0.20661704242229462
Batch: 4. Loss: 0.2505471408367157
Batch: 5. Loss: 0.2502690851688385
Batch: 6. Loss: 0.21793587505817413
Batch: 7. Loss: 0.22377173602581024
Batch: 8. Loss: 0.2581980526447296
Batch: 9. Loss: 0.23737111687660217
Batch: 10. Loss: 0.24329248070716858
Batch: 11. Loss: 0.22422967851161957
Batch: 12. Loss: 0.2493114322423935
Batch: 13. Loss: 0.24853308498859406
Batch: 14. Loss: 0.20745620131492615
Batch: 15. Loss: 0.2131514698266983
-----------------------------------------
Epoch completed in 1180.0930006504059 s.
Mean loss: 0.23313404619693756

Starting Epoch: 28, at: 1536246111.7389584
=========================================
Batch: 1. Loss: 0.2544043958187103
Batch: 2. Loss: 0.21404528617858887
Batch: 3. Loss: 0.22375328838825226
Batch: 4. Loss: 0.220289945602417
Batch: 5. Loss: 0.23438861966133118
Batch: 6. Loss: 0.23868624866008759
Batch: 7. Loss: 0.21276679635047913
Batch: 8. Loss: 0.22148631513118744
Batch: 9. Loss: 0.22555182874202728
Batch: 10. Loss: 0.23138919472694397
Batch: 11. Loss: 0.22031977772712708
Batch: 12. Loss: 0.23684410750865936
Batch: 13. Loss: 0.2277640849351883
Batch: 14. Loss: 0.22301113605499268
Batch: 15. Loss: 0.24377919733524323
-----------------------------------------
Epoch completed in 1167.5302469730377 s.
Mean loss: 0.22856535017490387

Starting Epoch: 29, at: 1536247279.2692363
=========================================
Batch: 1. Loss: 0.22929197549819946
Batch: 2. Loss: 0.2066253274679184
Batch: 3. Loss: 0.1969519406557083
Batch: 4. Loss: 0.22616228461265564
Batch: 5. Loss: 0.22205303609371185
Batch: 6. Loss: 0.23137539625167847
Batch: 7. Loss: 0.21301917731761932
Batch: 8. Loss: 0.21342766284942627
Batch: 9. Loss: 0.23417019844055176
Batch: 10. Loss: 0.21388684213161469
Batch: 11. Loss: 0.19785237312316895
Batch: 12. Loss: 0.22335727512836456
Batch: 13. Loss: 0.2460656762123108
Batch: 14. Loss: 0.23185691237449646
Batch: 15. Loss: 0.2065844088792801
-----------------------------------------
Epoch completed in 1173.5320658683777 s.
Mean loss: 0.21951201558113098

Starting Epoch: 30, at: 1536248452.8013327
=========================================
Batch: 1. Loss: 0.2160290628671646
Batch: 2. Loss: 0.23074033856391907
Batch: 3. Loss: 0.21291065216064453
Batch: 4. Loss: 0.2292315661907196
Batch: 5. Loss: 0.21073517203330994
Batch: 6. Loss: 0.24347524344921112
Batch: 7. Loss: 0.21335548162460327
Batch: 8. Loss: 0.20606856048107147
Batch: 9. Loss: 0.2109490931034088
Batch: 10. Loss: 0.1943041980266571
Batch: 11. Loss: 0.2138633280992508
Batch: 12. Loss: 0.21293134987354279
Batch: 13. Loss: 0.22648803889751434
Batch: 14. Loss: 0.21973291039466858
Batch: 15. Loss: 0.2098933607339859
-----------------------------------------
Epoch completed in 1172.7710404396057 s.
Mean loss: 0.21671390533447266

Starting Epoch: 31, at: 1536249625.5724044
=========================================
Batch: 1. Loss: 0.20866098999977112
Batch: 2. Loss: 0.1960594356060028
Batch: 3. Loss: 0.19757552444934845
Batch: 4. Loss: 0.2044597715139389
Batch: 5. Loss: 0.24555008113384247
Batch: 6. Loss: 0.20384490489959717
Batch: 7. Loss: 0.23977118730545044
Batch: 8. Loss: 0.19946324825286865
Batch: 9. Loss: 0.22986730933189392
Batch: 10. Loss: 0.2140471339225769
Batch: 11. Loss: 0.18390564620494843
Batch: 12. Loss: 0.26328417658805847
Batch: 13. Loss: 0.21490027010440826
Batch: 14. Loss: 0.2104770690202713
Batch: 15. Loss: 0.23555411398410797
-----------------------------------------
Epoch completed in 1166.118017911911 s.
Mean loss: 0.21649473905563354

Starting Epoch: 32, at: 1536250791.6904614
=========================================
Batch: 1. Loss: 0.20754635334014893
Batch: 2. Loss: 0.19880902767181396
Batch: 3. Loss: 0.2149362862110138
Batch: 4. Loss: 0.19249629974365234
Batch: 5. Loss: 0.21134595572948456
Batch: 6. Loss: 0.24214395880699158
Batch: 7. Loss: 0.2068558931350708
Batch: 8. Loss: 0.19430381059646606
Batch: 9. Loss: 0.20511798560619354
Batch: 10. Loss: 0.19540058076381683
Batch: 11. Loss: 0.23908483982086182
Batch: 12. Loss: 0.23483143746852875
Batch: 13. Loss: 0.21963775157928467
Batch: 14. Loss: 0.198923259973526
Batch: 15. Loss: 0.1887792944908142
-----------------------------------------
Epoch completed in 1169.623878479004 s.
Mean loss: 0.21001416444778442

Starting Epoch: 33, at: 1536251961.3143709
=========================================
Batch: 1. Loss: 0.20174825191497803
Batch: 2. Loss: 0.2135922908782959
Batch: 3. Loss: 0.20885881781578064
Batch: 4. Loss: 0.23210559785366058
Batch: 5. Loss: 0.23889705538749695
Batch: 6. Loss: 0.17400264739990234
Batch: 7. Loss: 0.1934129297733307
Batch: 8. Loss: 0.23162926733493805
Batch: 9. Loss: 0.16909511387348175
Batch: 10. Loss: 0.2034549117088318
Batch: 11. Loss: 0.20940883457660675
Batch: 12. Loss: 0.21180219948291779
Batch: 13. Loss: 0.20009975135326385
Batch: 14. Loss: 0.20574788749217987
Batch: 15. Loss: 0.1983582228422165
-----------------------------------------
Epoch completed in 1168.3335864543915 s.
Mean loss: 0.20614758133888245

Starting Epoch: 34, at: 1536253129.647987
=========================================
Batch: 1. Loss: 0.1931099146604538
Batch: 2. Loss: 0.211497962474823
Batch: 3. Loss: 0.1941034346818924
Batch: 4. Loss: 0.19281090795993805
Batch: 5. Loss: 0.21399806439876556
Batch: 6. Loss: 0.1909756064414978
Batch: 7. Loss: 0.20685319602489471
Batch: 8. Loss: 0.18669378757476807
Batch: 9. Loss: 0.1895735114812851
Batch: 10. Loss: 0.18984870612621307
Batch: 11. Loss: 0.1658657044172287
Batch: 12. Loss: 0.23059135675430298
Batch: 13. Loss: 0.20433303713798523
Batch: 14. Loss: 0.1885807365179062
Batch: 15. Loss: 0.18342338502407074
-----------------------------------------
Epoch completed in 1166.858736038208 s.
Mean loss: 0.19615061581134796

Starting Epoch: 35, at: 1536254296.506758
=========================================
Batch: 1. Loss: 0.18920324742794037
Batch: 2. Loss: 0.18294565379619598
Batch: 3. Loss: 0.1645507514476776
Batch: 4. Loss: 0.20487624406814575
Batch: 5. Loss: 0.1900997906923294
Batch: 6. Loss: 0.20911289751529694
Batch: 7. Loss: 0.18600349128246307
Batch: 8. Loss: 0.20612652599811554
Batch: 9. Loss: 0.20063522458076477
Batch: 10. Loss: 0.17499922215938568
Batch: 11. Loss: 0.19819602370262146
Batch: 12. Loss: 0.20142316818237305
Batch: 13. Loss: 0.2026176005601883
Batch: 14. Loss: 0.20380659401416779
Batch: 15. Loss: 0.1784762740135193
-----------------------------------------
Epoch completed in 1165.7722299098969 s.
Mean loss: 0.19287152588367462

Starting Epoch: 36, at: 1536255462.2790184
=========================================
Batch: 1. Loss: 0.1975870281457901
Batch: 2. Loss: 0.22202330827713013
Batch: 3. Loss: 0.17949490249156952
Batch: 4. Loss: 0.19404847919940948
Batch: 5. Loss: 0.1770455241203308
Batch: 6. Loss: 0.18378926813602448
Batch: 7. Loss: 0.1963721364736557
Batch: 8. Loss: 0.2109747678041458
Batch: 9. Loss: 0.20489639043807983
Batch: 10. Loss: 0.18138593435287476
Batch: 11. Loss: 0.22032345831394196
Batch: 12. Loss: 0.1727251410484314
Batch: 13. Loss: 0.1757553070783615
Batch: 14. Loss: 0.16298824548721313
Batch: 15. Loss: 0.20071791112422943
-----------------------------------------
Epoch completed in 1173.6858372688293 s.
Mean loss: 0.19200852513313293

Starting Epoch: 37, at: 1536256635.964884
=========================================
Batch: 1. Loss: 0.17808003723621368
Batch: 2. Loss: 0.18836165964603424
Batch: 3. Loss: 0.1853218525648117
Batch: 4. Loss: 0.18060295283794403
Batch: 5. Loss: 0.18569377064704895
Batch: 6. Loss: 0.1326015293598175
Batch: 7. Loss: 0.18546678125858307
Batch: 8. Loss: 0.20010338723659515
Batch: 9. Loss: 0.18530511856079102
Batch: 10. Loss: 0.1868978589773178
Batch: 11. Loss: 0.19198882579803467
Batch: 12. Loss: 0.21419665217399597
Batch: 13. Loss: 0.21666382253170013
Batch: 14. Loss: 0.16483651101589203
Batch: 15. Loss: 0.1797495186328888
-----------------------------------------
Epoch completed in 1168.008466243744 s.
Mean loss: 0.18505802750587463

Starting Epoch: 38, at: 1536257803.9733837
=========================================
Batch: 1. Loss: 0.21928009390830994
Batch: 2. Loss: 0.18160326778888702
Batch: 3. Loss: 0.19544357061386108
Batch: 4. Loss: 0.18736059963703156
Batch: 5. Loss: 0.17192718386650085
Batch: 6. Loss: 0.1911925971508026
Batch: 7. Loss: 0.22698688507080078
Batch: 8. Loss: 0.155909925699234
Batch: 9. Loss: 0.25406262278556824
Batch: 10. Loss: 0.22358572483062744
Batch: 11. Loss: 0.19018656015396118
Batch: 12. Loss: 0.17268197238445282
Batch: 13. Loss: 0.2164248526096344
Batch: 14. Loss: 0.206582173705101
Batch: 15. Loss: 0.21107566356658936
-----------------------------------------
Epoch completed in 1168.7715244293213 s.
Mean loss: 0.2002868950366974

Starting Epoch: 39, at: 1536258972.7449367
=========================================
Batch: 1. Loss: 0.2061033844947815
Batch: 2. Loss: 0.20075185596942902
Batch: 3. Loss: 0.1874818056821823
Batch: 4. Loss: 0.17886841297149658
Batch: 5. Loss: 0.14275506138801575
Batch: 6. Loss: 0.23805013298988342
Batch: 7. Loss: 0.18629556894302368
Batch: 8. Loss: 0.20506073534488678
Batch: 9. Loss: 0.24357594549655914
Batch: 10. Loss: 0.1595391482114792
Batch: 11. Loss: 0.1718321442604065
Batch: 12. Loss: 0.1833784580230713
Batch: 13. Loss: 0.20380806922912598
Batch: 14. Loss: 0.19365717470645905
Batch: 15. Loss: 0.21930108964443207
-----------------------------------------
Epoch completed in 1170.5589289665222 s.
Mean loss: 0.19469724595546722

Starting Epoch: 40, at: 1536260143.3039048
=========================================
Batch: 1. Loss: 0.2008390575647354
Batch: 2. Loss: 0.18251366913318634
Batch: 3. Loss: 0.18220479786396027
Batch: 4. Loss: 0.1746789962053299
Batch: 5. Loss: 0.16655661165714264
Batch: 6. Loss: 0.16351695358753204
Batch: 7. Loss: 0.1674218773841858
Batch: 8. Loss: 0.1703406423330307
Batch: 9. Loss: 0.1872008591890335
Batch: 10. Loss: 0.16840779781341553
Batch: 11. Loss: 0.19029957056045532
Batch: 12. Loss: 0.16574059426784515
Batch: 13. Loss: 0.21256780624389648
Batch: 14. Loss: 0.16994228959083557
Batch: 15. Loss: 0.20140480995178223
-----------------------------------------
Epoch completed in 1171.7666840553284 s.
Mean loss: 0.18024243414402008

Starting Epoch: 41, at: 1536261315.0706213
=========================================
Batch: 1. Loss: 0.17395509779453278
Batch: 2. Loss: 0.18630436062812805
Batch: 3. Loss: 0.20044520497322083
Batch: 4. Loss: 0.16604790091514587
Batch: 5. Loss: 0.19509945809841156
Batch: 6. Loss: 0.17673872411251068
Batch: 7. Loss: 0.16273976862430573
Batch: 8. Loss: 0.16405276954174042
Batch: 9. Loss: 0.16939979791641235
Batch: 10. Loss: 0.18411020934581757
Batch: 11. Loss: 0.17097333073616028
Batch: 12. Loss: 0.1554589718580246
Batch: 13. Loss: 0.19813266396522522
Batch: 14. Loss: 0.1660989373922348
Batch: 15. Loss: 0.1684132069349289
-----------------------------------------
Epoch completed in 1172.188767194748 s.
Mean loss: 0.17586468160152435

Starting Epoch: 42, at: 1536262487.2594235
=========================================
Batch: 1. Loss: 0.16572916507720947
Batch: 2. Loss: 0.16258391737937927
Batch: 3. Loss: 0.1800762563943863
Batch: 4. Loss: 0.19428277015686035
Batch: 5. Loss: 0.15783917903900146
Batch: 6. Loss: 0.2030782401561737
Batch: 7. Loss: 0.1492626667022705
Batch: 8. Loss: 0.16264061629772186
Batch: 9. Loss: 0.15423056483268738
Batch: 10. Loss: 0.15134204924106598
Batch: 11. Loss: 0.17189542949199677
Batch: 12. Loss: 0.16890855133533478
Batch: 13. Loss: 0.1795993596315384
Batch: 14. Loss: 0.19181208312511444
Batch: 15. Loss: 0.14464111626148224
-----------------------------------------
Epoch completed in 1171.975304365158 s.
Mean loss: 0.1691947877407074

Starting Epoch: 43, at: 1536263659.2347572
=========================================
Batch: 1. Loss: 0.16313166916370392
Batch: 2. Loss: 0.17876788973808289
Batch: 3. Loss: 0.15745514631271362
Batch: 4. Loss: 0.1592990905046463
Batch: 5. Loss: 0.17913475632667542
Batch: 6. Loss: 0.16821938753128052
Batch: 7. Loss: 0.1591886430978775
Batch: 8. Loss: 0.16776108741760254
Batch: 9. Loss: 0.16476185619831085
Batch: 10. Loss: 0.1577230840921402
Batch: 11. Loss: 0.1633426547050476
Batch: 12. Loss: 0.20901010930538177
Batch: 13. Loss: 0.13955058157444
Batch: 14. Loss: 0.17319504916667938
Batch: 15. Loss: 0.1534857153892517
-----------------------------------------
Epoch completed in 1174.9564852714539 s.
Mean loss: 0.1662684828042984

Starting Epoch: 44, at: 1536264834.1912723
=========================================
Batch: 1. Loss: 0.16144464910030365
Batch: 2. Loss: 0.16081678867340088
Batch: 3. Loss: 0.17967234551906586
Batch: 4. Loss: 0.1658753603696823
Batch: 5. Loss: 0.15466426312923431
Batch: 6. Loss: 0.13960842788219452
Batch: 7. Loss: 0.19012896716594696
Batch: 8. Loss: 0.16384297609329224
Batch: 9. Loss: 0.16803213953971863
Batch: 10. Loss: 0.16733035445213318
Batch: 11. Loss: 0.15966472029685974
Batch: 12. Loss: 0.181444451212883
Batch: 13. Loss: 0.18222206830978394
Batch: 14. Loss: 0.1503513604402542
Batch: 15. Loss: 0.1377181112766266
-----------------------------------------
Epoch completed in 1168.8936126232147 s.
Mean loss: 0.16418780386447906

Starting Epoch: 45, at: 1536266003.084915
=========================================
Batch: 1. Loss: 0.18770083785057068
Batch: 2. Loss: 0.2201002985239029
Batch: 3. Loss: 0.13651295006275177
Batch: 4. Loss: 0.17802371084690094
Batch: 5. Loss: 0.2019502818584442
Batch: 6. Loss: 0.15174540877342224
Batch: 7. Loss: 0.19868126511573792
Batch: 8. Loss: 0.1884986311197281
Batch: 9. Loss: 0.1616191565990448
Batch: 10. Loss: 0.1917532980442047
Batch: 11. Loss: 0.15856535732746124
Batch: 12. Loss: 0.18114647269248962
Batch: 13. Loss: 0.1738041490316391
Batch: 14. Loss: 0.16471296548843384
Batch: 15. Loss: 0.1744242012500763
-----------------------------------------
Epoch completed in 1173.1477234363556 s.
Mean loss: 0.17794924974441528

Starting Epoch: 46, at: 1536267176.2326958
=========================================
Batch: 1. Loss: 0.17684054374694824
Batch: 2. Loss: 0.16551557183265686
Batch: 3. Loss: 0.1424752026796341
Batch: 4. Loss: 0.16264154016971588
Batch: 5. Loss: 0.15615352988243103
Batch: 6. Loss: 0.17513665556907654
Batch: 7. Loss: 0.15070106089115143
Batch: 8. Loss: 0.1446489840745926
Batch: 9. Loss: 0.17106479406356812
Batch: 10. Loss: 0.16680985689163208
Batch: 11. Loss: 0.1614006906747818
Batch: 12. Loss: 0.16497191786766052
Batch: 13. Loss: 0.16510166227817535
Batch: 14. Loss: 0.1858232617378235
Batch: 15. Loss: 0.15342271327972412
-----------------------------------------
Epoch completed in 1174.9561166763306 s.
Mean loss: 0.16284720599651337

Starting Epoch: 47, at: 1536268351.1888425
=========================================
Batch: 1. Loss: 0.17197100818157196
Batch: 2. Loss: 0.14988745748996735
Batch: 3. Loss: 0.17131468653678894
Batch: 4. Loss: 0.14816579222679138
Batch: 5. Loss: 0.1690439134836197
Batch: 6. Loss: 0.1747499257326126
Batch: 7. Loss: 0.1704232096672058
Batch: 8. Loss: 0.14792215824127197
Batch: 9. Loss: 0.15769606828689575
Batch: 10. Loss: 0.15546481311321259
Batch: 11. Loss: 0.16744281351566315
Batch: 12. Loss: 0.1607864499092102
Batch: 13. Loss: 0.147910013794899
Batch: 14. Loss: 0.13836035132408142
Batch: 15. Loss: 0.2081349492073059
-----------------------------------------
Epoch completed in 1176.3575258255005 s.
Mean loss: 0.1626182347536087

Starting Epoch: 48, at: 1536269527.5464015
=========================================
Batch: 1. Loss: 0.17611175775527954
Batch: 2. Loss: 0.1617957502603531
Batch: 3. Loss: 0.16657772660255432
Batch: 4. Loss: 0.1600862443447113
Batch: 5. Loss: 0.142255961894989
Batch: 6. Loss: 0.17863596975803375
Batch: 7. Loss: 0.1731177717447281
Batch: 8. Loss: 0.15883786976337433
Batch: 9. Loss: 0.1760728806257248
Batch: 10. Loss: 0.1335441917181015
Batch: 11. Loss: 0.15755614638328552
Batch: 12. Loss: 0.1423361599445343
Batch: 13. Loss: 0.17955462634563446
Batch: 14. Loss: 0.17764566838741302
Batch: 15. Loss: 0.19880029559135437
-----------------------------------------
Epoch completed in 1176.37216258049 s.
Mean loss: 0.16552859544754028

Starting Epoch: 49, at: 1536270703.918601
=========================================
Batch: 1. Loss: 0.1860966980457306
Batch: 2. Loss: 0.17767202854156494
Batch: 3. Loss: 0.15189045667648315
Batch: 4. Loss: 0.1424075812101364
Batch: 5. Loss: 0.16188374161720276
Batch: 6. Loss: 0.1520223617553711
Batch: 7. Loss: 0.1612858921289444
Batch: 8. Loss: 0.16603122651576996
Batch: 9. Loss: 0.15580762922763824
Batch: 10. Loss: 0.14568743109703064
Batch: 11. Loss: 0.15990325808525085
Batch: 12. Loss: 0.12648411095142365
Batch: 13. Loss: 0.19569338858127594
Batch: 14. Loss: 0.17265290021896362
Batch: 15. Loss: 0.1565919816493988
-----------------------------------------
Epoch completed in 1176.2380411624908 s.
Mean loss: 0.16080738604068756

Starting Epoch: 50, at: 1536271880.1566803
=========================================
Batch: 1. Loss: 0.14722171425819397
Batch: 2. Loss: 0.1399519443511963
Batch: 3. Loss: 0.1681361049413681
Batch: 4. Loss: 0.1381121426820755
Batch: 5. Loss: 0.15575933456420898
Batch: 6. Loss: 0.16013675928115845
Batch: 7. Loss: 0.13494034111499786
Batch: 8. Loss: 0.1404382884502411
Batch: 9. Loss: 0.1468045860528946
Batch: 10. Loss: 0.18245729804039001
Batch: 11. Loss: 0.1681506633758545
Batch: 12. Loss: 0.17896555364131927
Batch: 13. Loss: 0.16232198476791382
Batch: 14. Loss: 0.13096889853477478
Batch: 15. Loss: 0.14603906869888306
-----------------------------------------
Epoch completed in 1174.760449886322 s.
Mean loss: 0.1533603072166443

Training Finished. Saving test images to: ./runs/1536273054.9185991
