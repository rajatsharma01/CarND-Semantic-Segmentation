/home/rajat/miniconda3/envs/carnd-term1/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
TensorFlow Version: 1.10.0
main.py:27: UserWarning: No GPU found. Please use a GPU to train your neural network.
  warnings.warn('No GPU found. Please use a GPU to train your neural network.')
Tests Passed
Tests Passed
Tests Passed
Tests Passed
Tests Passed
2018-09-09 11:14:24.152557: W tensorflow/core/framework/allocator.cc:113] Allocation of 411041792 exceeds 10% of system memory.
2018-09-09 11:14:25.090898: W tensorflow/core/framework/allocator.cc:113] Allocation of 411041792 exceeds 10% of system memory.
2018-09-09 11:14:28.097146: W tensorflow/core/framework/allocator.cc:113] Allocation of 411041792 exceeds 10% of system memory.
2018-09-09 11:14:28.561312: W tensorflow/core/framework/allocator.cc:113] Allocation of 411041792 exceeds 10% of system memory.
2018-09-09 11:14:28.730129: W tensorflow/core/framework/allocator.cc:113] Allocation of 411041792 exceeds 10% of system memory.
Starting Epoch: 1/25, at: 1536516888.6572058
=========================================
Batch: 1. Loss: 0.6933287978172302
Batch: 2. Loss: 2.9589056968688965
Batch: 3. Loss: 0.6761511564254761
Batch: 4. Loss: 0.7460778951644897
Batch: 5. Loss: 0.7134632468223572
Batch: 6. Loss: 0.637292742729187
Batch: 7. Loss: 0.5225504636764526
Batch: 8. Loss: 0.50067138671875
Batch: 9. Loss: 0.5180982947349548
Batch: 10. Loss: 0.4442886412143707
Batch: 11. Loss: 0.4726487994194031
Batch: 12. Loss: 0.45192039012908936
Batch: 13. Loss: 0.45828887820243835
Batch: 14. Loss: 0.43940576910972595
Batch: 15. Loss: 0.3710481822490692
Batch: 16. Loss: 0.5124491453170776
Batch: 17. Loss: 0.33199945092201233
Batch: 18. Loss: 0.382744163274765
Batch: 19. Loss: 0.418861448764801
Batch: 20. Loss: 0.38650181889533997
Batch: 21. Loss: 0.34701454639434814
Batch: 22. Loss: 0.29690828919410706
Batch: 23. Loss: 0.4846762716770172
Batch: 24. Loss: 0.3236633241176605
Batch: 25. Loss: 0.3179585933685303
Batch: 26. Loss: 0.2497716099023819
Batch: 27. Loss: 0.309826523065567
Batch: 28. Loss: 0.2588474452495575
Batch: 29. Loss: 0.33023378252983093
Batch: 30. Loss: 0.25068724155426025
Batch: 31. Loss: 0.22782815992832184
Batch: 32. Loss: 0.24319717288017273
Batch: 33. Loss: 0.2413332313299179
Batch: 34. Loss: 0.24310821294784546
Batch: 35. Loss: 0.23429515957832336
Batch: 36. Loss: 0.16501222550868988
Batch: 37. Loss: 0.16048410534858704
-----------------------------------------
Epoch completed in 989.325766324997 s.
Mean loss: 0.4681498110294342

Starting Epoch: 2/25, at: 1536517877.9830053
=========================================
Batch: 1. Loss: 0.4199973940849304
Batch: 2. Loss: 0.3495013117790222
Batch: 3. Loss: 0.39021894335746765
Batch: 4. Loss: 0.23778848350048065
Batch: 5. Loss: 0.24504812061786652
Batch: 6. Loss: 0.2448931783437729
Batch: 7. Loss: 0.3276979327201843
Batch: 8. Loss: 0.30267101526260376
Batch: 9. Loss: 0.25942352414131165
Batch: 10. Loss: 0.21656155586242676
Batch: 11. Loss: 0.22102639079093933
Batch: 12. Loss: 0.28848499059677124
Batch: 13. Loss: 0.23829621076583862
Batch: 14. Loss: 0.18398624658584595
Batch: 15. Loss: 0.22191157937049866
Batch: 16. Loss: 0.16329510509967804
Batch: 17. Loss: 0.13647039234638214
Batch: 18. Loss: 0.16349656879901886
Batch: 19. Loss: 0.20153291523456573
Batch: 20. Loss: 0.1924871951341629
Batch: 21. Loss: 0.20683255791664124
Batch: 22. Loss: 0.24378825724124908
Batch: 23. Loss: 0.17894196510314941
Batch: 24. Loss: 0.16547851264476776
Batch: 25. Loss: 0.19045117497444153
Batch: 26. Loss: 0.19586902856826782
Batch: 27. Loss: 0.15250830352306366
Batch: 28. Loss: 0.1818724274635315
Batch: 29. Loss: 0.13691923022270203
Batch: 30. Loss: 0.1983913630247116
Batch: 31. Loss: 0.2055356502532959
Batch: 32. Loss: 0.20571886003017426
Batch: 33. Loss: 0.19751910865306854
Batch: 34. Loss: 0.14414602518081665
Batch: 35. Loss: 0.1619347482919693
Batch: 36. Loss: 0.13332200050354004
Batch: 37. Loss: 0.12134311348199844
-----------------------------------------
Epoch completed in 978.8338551521301 s.
Mean loss: 0.21690164506435394

Starting Epoch: 3/25, at: 1536518856.8168912
=========================================
Batch: 1. Loss: 0.15158501267433167
Batch: 2. Loss: 0.16338534653186798
Batch: 3. Loss: 0.17015482485294342
Batch: 4. Loss: 0.17350071668624878
Batch: 5. Loss: 0.16549640893936157
Batch: 6. Loss: 0.1254531443119049
Batch: 7. Loss: 0.14407877624034882
Batch: 8. Loss: 0.13491304218769073
Batch: 9. Loss: 0.26366135478019714
Batch: 10. Loss: 0.1663575917482376
Batch: 11. Loss: 0.16614988446235657
Batch: 12. Loss: 0.2098665088415146
Batch: 13. Loss: 0.1843179613351822
Batch: 14. Loss: 0.18125498294830322
Batch: 15. Loss: 0.15265823900699615
Batch: 16. Loss: 0.24621744453907013
Batch: 17. Loss: 0.16363264620304108
Batch: 18. Loss: 0.10857930779457092
Batch: 19. Loss: 0.18726864457130432
Batch: 20. Loss: 0.17007505893707275
Batch: 21. Loss: 0.19745178520679474
Batch: 22. Loss: 0.15075868368148804
Batch: 23. Loss: 0.15228314697742462
Batch: 24. Loss: 0.1878775954246521
Batch: 25. Loss: 0.14422258734703064
Batch: 26. Loss: 0.1337922066450119
Batch: 27. Loss: 0.1738634556531906
Batch: 28. Loss: 0.15535983443260193
Batch: 29. Loss: 0.12711574137210846
Batch: 30. Loss: 0.15513598918914795
Batch: 31. Loss: 0.14744117856025696
Batch: 32. Loss: 0.1602383702993393
Batch: 33. Loss: 0.12374730408191681
Batch: 34. Loss: 0.13226214051246643
Batch: 35. Loss: 0.139733225107193
Batch: 36. Loss: 0.14085128903388977
Batch: 37. Loss: 0.10808984190225601
-----------------------------------------
Epoch completed in 967.1257545948029 s.
Mean loss: 0.1610495150089264

Starting Epoch: 4/25, at: 1536519823.942679
=========================================
Batch: 1. Loss: 0.18899932503700256
Batch: 2. Loss: 0.17906147241592407
Batch: 3. Loss: 0.23219747841358185
Batch: 4. Loss: 0.12738311290740967
Batch: 5. Loss: 0.14963604509830475
Batch: 6. Loss: 0.1594826877117157
Batch: 7. Loss: 0.14363867044448853
Batch: 8. Loss: 0.13742150366306305
Batch: 9. Loss: 0.14472264051437378
Batch: 10. Loss: 0.13540096580982208
Batch: 11. Loss: 0.14803089201450348
Batch: 12. Loss: 0.1364785134792328
Batch: 13. Loss: 0.14358273148536682
Batch: 14. Loss: 0.14847618341445923
Batch: 15. Loss: 0.10817550867795944
Batch: 16. Loss: 0.15897466242313385
Batch: 17. Loss: 0.11914245784282684
Batch: 18. Loss: 0.10518721491098404
Batch: 19. Loss: 0.10353536903858185
Batch: 20. Loss: 0.1373031884431839
Batch: 21. Loss: 0.12977249920368195
Batch: 22. Loss: 0.23201750218868256
Batch: 23. Loss: 0.11405979096889496
Batch: 24. Loss: 0.14577117562294006
Batch: 25. Loss: 0.13121487200260162
Batch: 26. Loss: 0.15817517042160034
Batch: 27. Loss: 0.14227455854415894
Batch: 28. Loss: 0.10372034460306168
Batch: 29. Loss: 0.1322924643754959
Batch: 30. Loss: 0.10459627211093903
Batch: 31. Loss: 0.09956049174070358
Batch: 32. Loss: 0.17817915976047516
Batch: 33. Loss: 0.11916008591651917
Batch: 34. Loss: 0.11849430948495865
Batch: 35. Loss: 0.14810192584991455
Batch: 36. Loss: 0.09640228003263474
Batch: 37. Loss: 0.04498758167028427
-----------------------------------------
Epoch completed in 948.1823160648346 s.
Mean loss: 0.13798949122428894

Starting Epoch: 5/25, at: 1536520772.1250248
=========================================
Batch: 1. Loss: 0.12697063386440277
Batch: 2. Loss: 0.132688969373703
Batch: 3. Loss: 0.1271829456090927
Batch: 4. Loss: 0.12494401633739471
Batch: 5. Loss: 0.1608349084854126
Batch: 6. Loss: 0.11669114232063293
Batch: 7. Loss: 0.12213146686553955
Batch: 8. Loss: 0.13206760585308075
Batch: 9. Loss: 0.11842463910579681
Batch: 10. Loss: 0.13557422161102295
Batch: 11. Loss: 0.10592233389616013
Batch: 12. Loss: 0.12901438772678375
Batch: 13. Loss: 0.10617128014564514
Batch: 14. Loss: 0.10832251608371735
Batch: 15. Loss: 0.09420508146286011
Batch: 16. Loss: 0.14913275837898254
Batch: 17. Loss: 0.08212552219629288
Batch: 18. Loss: 0.14895844459533691
Batch: 19. Loss: 0.10536741465330124
Batch: 20. Loss: 0.09762319922447205
Batch: 21. Loss: 0.1456596404314041
Batch: 22. Loss: 0.09643328189849854
Batch: 23. Loss: 0.10769711434841156
Batch: 24. Loss: 0.12615448236465454
Batch: 25. Loss: 0.11733831465244293
Batch: 26. Loss: 0.10501550137996674
Batch: 27. Loss: 0.09779414534568787
Batch: 28. Loss: 0.12251461297273636
Batch: 29. Loss: 0.08784515410661697
Batch: 30. Loss: 0.11109037697315216
Batch: 31. Loss: 0.16682986915111542
Batch: 32. Loss: 0.1228899136185646
Batch: 33. Loss: 0.11940797418355942
Batch: 34. Loss: 0.11864878982305527
Batch: 35. Loss: 0.09656082838773727
Batch: 36. Loss: 0.09624467045068741
Batch: 37. Loss: 0.07935868948698044
-----------------------------------------
Epoch completed in 951.0490250587463 s.
Mean loss: 0.11734694242477417

Starting Epoch: 6/25, at: 1536521723.1740794
=========================================
Batch: 1. Loss: 0.10484107583761215
Batch: 2. Loss: 0.12619955837726593
Batch: 3. Loss: 0.09352535754442215
Batch: 4. Loss: 0.11086953431367874
Batch: 5. Loss: 0.12722446024417877
Batch: 6. Loss: 0.08257149904966354
Batch: 7. Loss: 0.11537609249353409
Batch: 8. Loss: 0.10356698930263519
Batch: 9. Loss: 0.11543867737054825
Batch: 10. Loss: 0.10158858448266983
Batch: 11. Loss: 0.08470281213521957
Batch: 12. Loss: 0.0903589203953743
Batch: 13. Loss: 0.11686692386865616
Batch: 14. Loss: 0.11596981436014175
Batch: 15. Loss: 0.1488211452960968
Batch: 16. Loss: 0.11474083364009857
Batch: 17. Loss: 0.09245314449071884
Batch: 18. Loss: 0.11748836189508438
Batch: 19. Loss: 0.08615957200527191
Batch: 20. Loss: 0.1069931611418724
Batch: 21. Loss: 0.09550782293081284
Batch: 22. Loss: 0.09670897573232651
Batch: 23. Loss: 0.13250641524791718
Batch: 24. Loss: 0.11598622798919678
Batch: 25. Loss: 0.12417344003915787
Batch: 26. Loss: 0.11167369037866592
Batch: 27. Loss: 0.10014932602643967
Batch: 28. Loss: 0.11775144189596176
Batch: 29. Loss: 0.09533889591693878
Batch: 30. Loss: 0.0805133655667305
Batch: 31. Loss: 0.09939964860677719
Batch: 32. Loss: 0.06283637136220932
Batch: 33. Loss: 0.12203884869813919
Batch: 34. Loss: 0.16704808175563812
Batch: 35. Loss: 0.10050045698881149
Batch: 36. Loss: 0.10945075005292892
Batch: 37. Loss: 0.036285653710365295
-----------------------------------------
Epoch completed in 946.2583975791931 s.
Mean loss: 0.10604394227266312

Starting Epoch: 7/25, at: 1536522669.432508
=========================================
Batch: 1. Loss: 0.1458507776260376
Batch: 2. Loss: 0.09029991179704666
Batch: 3. Loss: 0.101688452064991
Batch: 4. Loss: 0.09141803532838821
Batch: 5. Loss: 0.07531283795833588
Batch: 6. Loss: 0.1972358673810959
Batch: 7. Loss: 0.0821404978632927
Batch: 8. Loss: 0.09955515712499619
Batch: 9. Loss: 0.10401377081871033
Batch: 10. Loss: 0.13612185418605804
Batch: 11. Loss: 0.10343333333730698
Batch: 12. Loss: 0.10125446319580078
Batch: 13. Loss: 0.14219626784324646
Batch: 14. Loss: 0.10328060388565063
Batch: 15. Loss: 0.11561795324087143
Batch: 16. Loss: 0.07222245633602142
Batch: 17. Loss: 0.07888917624950409
Batch: 18. Loss: 0.09407299757003784
Batch: 19. Loss: 0.11090391874313354
Batch: 20. Loss: 0.09135861694812775
Batch: 21. Loss: 0.10062403976917267
Batch: 22. Loss: 0.07984323054552078
Batch: 23. Loss: 0.09377865493297577
Batch: 24. Loss: 0.11307084560394287
Batch: 25. Loss: 0.08415888994932175
Batch: 26. Loss: 0.08676161617040634
Batch: 27. Loss: 0.08550843596458435
Batch: 28. Loss: 0.0844867080450058
Batch: 29. Loss: 0.10898526757955551
Batch: 30. Loss: 0.07917851209640503
Batch: 31. Loss: 0.09019409865140915
Batch: 32. Loss: 0.0850905328989029
Batch: 33. Loss: 0.09371154010295868
Batch: 34. Loss: 0.07577164471149445
Batch: 35. Loss: 0.10064851492643356
Batch: 36. Loss: 0.08548317104578018
Batch: 37. Loss: 0.1049332544207573
-----------------------------------------
Epoch completed in 937.9809601306915 s.
Mean loss: 0.0997052937746048

Starting Epoch: 8/25, at: 1536523607.4135134
=========================================
Batch: 1. Loss: 0.08862250298261642
Batch: 2. Loss: 0.0781865268945694
Batch: 3. Loss: 0.17448851466178894
Batch: 4. Loss: 0.10227790474891663
Batch: 5. Loss: 0.0997539758682251
Batch: 6. Loss: 0.09782064706087112
Batch: 7. Loss: 0.10559214651584625
Batch: 8. Loss: 0.09645383059978485
Batch: 9. Loss: 0.12778347730636597
Batch: 10. Loss: 0.127252995967865
Batch: 11. Loss: 0.11811485886573792
Batch: 12. Loss: 0.10134213417768478
Batch: 13. Loss: 0.13739971816539764
Batch: 14. Loss: 0.12094979733228683
Batch: 15. Loss: 0.09403270483016968
Batch: 16. Loss: 0.10496816784143448
Batch: 17. Loss: 0.08725399523973465
Batch: 18. Loss: 0.11238561570644379
Batch: 19. Loss: 0.09796677529811859
Batch: 20. Loss: 0.10771384090185165
Batch: 21. Loss: 0.07911775261163712
Batch: 22. Loss: 0.09888695180416107
Batch: 23. Loss: 0.11949858069419861
Batch: 24. Loss: 0.18122678995132446
Batch: 25. Loss: 0.1356351226568222
Batch: 26. Loss: 0.10845240205526352
Batch: 27. Loss: 0.09780032187700272
Batch: 28. Loss: 0.10712740570306778
Batch: 29. Loss: 0.10662408918142319
Batch: 30. Loss: 0.07029450684785843
Batch: 31. Loss: 0.18250173330307007
Batch: 32. Loss: 0.13952933251857758
Batch: 33. Loss: 0.113570436835289
Batch: 34. Loss: 0.1222190111875534
Batch: 35. Loss: 0.12192421406507492
Batch: 36. Loss: 0.07381206005811691
Batch: 37. Loss: 0.11263204365968704
-----------------------------------------
Epoch completed in 947.5725214481354 s.
Mean loss: 0.11219494789838791

Starting Epoch: 9/25, at: 1536524554.9860616
=========================================
Batch: 1. Loss: 0.08729960024356842
Batch: 2. Loss: 0.09251940995454788
Batch: 3. Loss: 0.09706048667430878
Batch: 4. Loss: 0.07746405154466629
Batch: 5. Loss: 0.09519866108894348
Batch: 6. Loss: 0.09992814064025879
Batch: 7. Loss: 0.10465352982282639
Batch: 8. Loss: 0.05771216005086899
Batch: 9. Loss: 0.1665354073047638
Batch: 10. Loss: 0.09283033758401871
Batch: 11. Loss: 0.10107738524675369
Batch: 12. Loss: 0.11984265595674515
Batch: 13. Loss: 0.09638452529907227
Batch: 14. Loss: 0.1074729785323143
Batch: 15. Loss: 0.12367109209299088
Batch: 16. Loss: 0.14338672161102295
Batch: 17. Loss: 0.09857111424207687
Batch: 18. Loss: 0.11321527510881424
Batch: 19. Loss: 0.09540042281150818
Batch: 20. Loss: 0.09098711609840393
Batch: 21. Loss: 0.12724456191062927
Batch: 22. Loss: 0.11773739755153656
Batch: 23. Loss: 0.11127310991287231
Batch: 24. Loss: 0.12909777462482452
Batch: 25. Loss: 0.11729449778795242
Batch: 26. Loss: 0.15004561841487885
Batch: 27. Loss: 0.09176073223352432
Batch: 28. Loss: 0.11371225118637085
Batch: 29. Loss: 0.11002524197101593
Batch: 30. Loss: 0.09602336585521698
Batch: 31. Loss: 0.08559522777795792
Batch: 32. Loss: 0.07961150258779526
Batch: 33. Loss: 0.0925343781709671
Batch: 34. Loss: 0.09627939760684967
Batch: 35. Loss: 0.11787133663892746
Batch: 36. Loss: 0.152621790766716
Batch: 37. Loss: 0.2384728640317917
-----------------------------------------
Epoch completed in 949.2686779499054 s.
Mean loss: 0.1104976162314415

Starting Epoch: 10/25, at: 1536525504.2547781
=========================================
Batch: 1. Loss: 0.1472337245941162
Batch: 2. Loss: 0.14361871778964996
Batch: 3. Loss: 0.12600845098495483
Batch: 4. Loss: 0.21150408685207367
Batch: 5. Loss: 0.12399297207593918
Batch: 6. Loss: 0.13333727419376373
Batch: 7. Loss: 0.1351931244134903
Batch: 8. Loss: 0.08973905444145203
Batch: 9. Loss: 0.10268859565258026
Batch: 10. Loss: 0.10088562220335007
Batch: 11. Loss: 0.12508180737495422
Batch: 12. Loss: 0.12683169543743134
Batch: 13. Loss: 0.07687121629714966
Batch: 14. Loss: 0.10594520717859268
Batch: 15. Loss: 0.07403957843780518
Batch: 16. Loss: 0.07909110188484192
Batch: 17. Loss: 0.09255816787481308
Batch: 18. Loss: 0.09029495716094971
Batch: 19. Loss: 0.11129583418369293
Batch: 20. Loss: 0.08882305771112442
Batch: 21. Loss: 0.0883350670337677
Batch: 22. Loss: 0.0862099751830101
Batch: 23. Loss: 0.09365126490592957
Batch: 24. Loss: 0.14448629319667816
Batch: 25. Loss: 0.06918492168188095
Batch: 26. Loss: 0.09001533687114716
Batch: 27. Loss: 0.08102289587259293
Batch: 28. Loss: 0.10260234773159027
Batch: 29. Loss: 0.0765460804104805
Batch: 30. Loss: 0.10783957690000534
Batch: 31. Loss: 0.12917973101139069
Batch: 32. Loss: 0.07495566457509995
Batch: 33. Loss: 0.10368314385414124
Batch: 34. Loss: 0.08657041937112808
Batch: 35. Loss: 0.059650346636772156
Batch: 36. Loss: 0.06563369929790497
Batch: 37. Loss: 0.15994375944137573
-----------------------------------------
Epoch completed in 955.7882776260376 s.
Mean loss: 0.10552824288606644

Starting Epoch: 11/25, at: 1536526460.0430923
=========================================
Batch: 1. Loss: 0.048847924917936325
Batch: 2. Loss: 0.0686594694852829
Batch: 3. Loss: 0.06706167757511139
Batch: 4. Loss: 0.07596080005168915
Batch: 5. Loss: 0.050140950828790665
Batch: 6. Loss: 0.13604263961315155
Batch: 7. Loss: 0.09466833621263504
Batch: 8. Loss: 0.07686733454465866
Batch: 9. Loss: 0.10171560198068619
Batch: 10. Loss: 0.08421817421913147
Batch: 11. Loss: 0.0741114616394043
Batch: 12. Loss: 0.08003535866737366
Batch: 13. Loss: 0.07350568473339081
Batch: 14. Loss: 0.08053965866565704
Batch: 15. Loss: 0.0700627863407135
Batch: 16. Loss: 0.06582445651292801
Batch: 17. Loss: 0.06646013259887695
Batch: 18. Loss: 0.07967369258403778
Batch: 19. Loss: 0.05188174545764923
Batch: 20. Loss: 0.05537019670009613
Batch: 21. Loss: 0.06903615593910217
Batch: 22. Loss: 0.11341437697410583
Batch: 23. Loss: 0.07848772406578064
Batch: 24. Loss: 0.08484805375337601
Batch: 25. Loss: 0.07173619419336319
Batch: 26. Loss: 0.07131654769182205
Batch: 27. Loss: 0.07512287050485611
Batch: 28. Loss: 0.08805325627326965
Batch: 29. Loss: 0.10040578246116638
Batch: 30. Loss: 0.08342728018760681
Batch: 31. Loss: 0.06943491846323013
Batch: 32. Loss: 0.07783206552267075
Batch: 33. Loss: 0.0786876454949379
Batch: 34. Loss: 0.07722974568605423
Batch: 35. Loss: 0.0797191932797432
Batch: 36. Loss: 0.08646159619092941
Batch: 37. Loss: 0.10256513208150864
-----------------------------------------
Epoch completed in 938.5313911437988 s.
Mean loss: 0.078633151948452

Starting Epoch: 12/25, at: 1536527398.5745022
=========================================
Batch: 1. Loss: 0.08312633633613586
Batch: 2. Loss: 0.07621674239635468
Batch: 3. Loss: 0.06844969093799591
Batch: 4. Loss: 0.08938492834568024
Batch: 5. Loss: 0.055027760565280914
Batch: 6. Loss: 0.0971880853176117
Batch: 7. Loss: 0.07267013192176819
Batch: 8. Loss: 0.07798586785793304
Batch: 9. Loss: 0.09117940068244934
Batch: 10. Loss: 0.06370263546705246
Batch: 11. Loss: 0.05928647145628929
Batch: 12. Loss: 0.06079909950494766
Batch: 13. Loss: 0.0692330151796341
Batch: 14. Loss: 0.07557220757007599
Batch: 15. Loss: 0.045083388686180115
Batch: 16. Loss: 0.06778062880039215
Batch: 17. Loss: 0.09719561040401459
Batch: 18. Loss: 0.05563422292470932
Batch: 19. Loss: 0.05699145793914795
Batch: 20. Loss: 0.09399256855249405
Batch: 21. Loss: 0.06787160038948059
Batch: 22. Loss: 0.07752081751823425
Batch: 23. Loss: 0.05445093289017677
Batch: 24. Loss: 0.08335717022418976
Batch: 25. Loss: 0.06834357976913452
Batch: 26. Loss: 0.05797048285603523
Batch: 27. Loss: 0.08227047324180603
Batch: 28. Loss: 0.09114240854978561
Batch: 29. Loss: 0.06389737129211426
Batch: 30. Loss: 0.08701920509338379
Batch: 31. Loss: 0.0535501167178154
Batch: 32. Loss: 0.07867967337369919
Batch: 33. Loss: 0.04750175401568413
Batch: 34. Loss: 0.06400569528341293
Batch: 35. Loss: 0.0703829750418663
Batch: 36. Loss: 0.0721200630068779
Batch: 37. Loss: 0.038187019526958466
-----------------------------------------
Epoch completed in 936.8164308071136 s.
Mean loss: 0.0706695020198822

Starting Epoch: 13/25, at: 1536528335.3909574
=========================================
Batch: 1. Loss: 0.05332294479012489
Batch: 2. Loss: 0.08394287526607513
Batch: 3. Loss: 0.04971037805080414
Batch: 4. Loss: 0.0532582625746727
Batch: 5. Loss: 0.048571642488241196
Batch: 6. Loss: 0.04643712192773819
Batch: 7. Loss: 0.06924170255661011
Batch: 8. Loss: 0.055924925953149796
Batch: 9. Loss: 0.07383263856172562
Batch: 10. Loss: 0.059302881360054016
Batch: 11. Loss: 0.07639919966459274
Batch: 12. Loss: 0.07504460215568542
Batch: 13. Loss: 0.06382878124713898
Batch: 14. Loss: 0.034363754093647
Batch: 15. Loss: 0.08651209622621536
Batch: 16. Loss: 0.07216887921094894
Batch: 17. Loss: 0.0697799101471901
Batch: 18. Loss: 0.0739193931221962
Batch: 19. Loss: 0.05709238350391388
Batch: 20. Loss: 0.0812002494931221
Batch: 21. Loss: 0.08225630223751068
Batch: 22. Loss: 0.07317580282688141
Batch: 23. Loss: 0.053205814212560654
Batch: 24. Loss: 0.10509295761585236
Batch: 25. Loss: 0.06535785645246506
Batch: 26. Loss: 0.06544642895460129
Batch: 27. Loss: 0.08066673576831818
Batch: 28. Loss: 0.07085654139518738
Batch: 29. Loss: 0.07187122851610184
Batch: 30. Loss: 0.06293388456106186
Batch: 31. Loss: 0.0694069042801857
Batch: 32. Loss: 0.04058699309825897
Batch: 33. Loss: 0.051999807357788086
Batch: 34. Loss: 0.09622278809547424
Batch: 35. Loss: 0.08430503308773041
Batch: 36. Loss: 0.04147886484861374
Batch: 37. Loss: 0.058529097586870193
-----------------------------------------
Epoch completed in 934.9320538043976 s.
Mean loss: 0.06641209870576859

Starting Epoch: 14/25, at: 1536529270.3230464
=========================================
Batch: 1. Loss: 0.06144982576370239
Batch: 2. Loss: 0.05868331715464592
Batch: 3. Loss: 0.06045136973261833
Batch: 4. Loss: 0.07016272097826004
Batch: 5. Loss: 0.07013259083032608
Batch: 6. Loss: 0.05844714492559433
Batch: 7. Loss: 0.061461471021175385
Batch: 8. Loss: 0.0471675805747509
Batch: 9. Loss: 0.052627891302108765
Batch: 10. Loss: 0.04777687042951584
Batch: 11. Loss: 0.06395436078310013
Batch: 12. Loss: 0.061736419796943665
Batch: 13. Loss: 0.05019767954945564
Batch: 14. Loss: 0.09385877847671509
Batch: 15. Loss: 0.06538406014442444
Batch: 16. Loss: 0.08899857848882675
Batch: 17. Loss: 0.06856286525726318
Batch: 18. Loss: 0.09044267237186432
Batch: 19. Loss: 0.08823855966329575
Batch: 20. Loss: 0.08170539140701294
Batch: 21. Loss: 0.06728927791118622
Batch: 22. Loss: 0.06761033833026886
Batch: 23. Loss: 0.0769248902797699
Batch: 24. Loss: 0.08201835304498672
Batch: 25. Loss: 0.05633721500635147
Batch: 26. Loss: 0.04551251232624054
Batch: 27. Loss: 0.04615003615617752
Batch: 28. Loss: 0.04454142600297928
Batch: 29. Loss: 0.07046505808830261
Batch: 30. Loss: 0.056850891560316086
Batch: 31. Loss: 0.07477111369371414
Batch: 32. Loss: 0.08263229578733444
Batch: 33. Loss: 0.07485856860876083
Batch: 34. Loss: 0.055155832320451736
Batch: 35. Loss: 0.10679363459348679
Batch: 36. Loss: 0.05761632323265076
Batch: 37. Loss: 0.09804419428110123
-----------------------------------------
Epoch completed in 936.4831359386444 s.
Mean loss: 0.06770303100347519

Starting Epoch: 15/25, at: 1536530206.8062127
=========================================
Batch: 1. Loss: 0.06975656002759933
Batch: 2. Loss: 0.07880356162786484
Batch: 3. Loss: 0.06857496500015259
Batch: 4. Loss: 0.05744491145014763
Batch: 5. Loss: 0.06187659502029419
Batch: 6. Loss: 0.047462720423936844
Batch: 7. Loss: 0.05245191603899002
Batch: 8. Loss: 0.0628555566072464
Batch: 9. Loss: 0.06766416132450104
Batch: 10. Loss: 0.05801953375339508
Batch: 11. Loss: 0.07849959284067154
Batch: 12. Loss: 0.06865879148244858
Batch: 13. Loss: 0.059185370802879333
Batch: 14. Loss: 0.04705382511019707
Batch: 15. Loss: 0.06380336731672287
Batch: 16. Loss: 0.037982601672410965
Batch: 17. Loss: 0.04684602469205856
Batch: 18. Loss: 0.05559030547738075
Batch: 19. Loss: 0.04620080441236496
Batch: 20. Loss: 0.059127360582351685
Batch: 21. Loss: 0.06640922278165817
Batch: 22. Loss: 0.05138688161969185
Batch: 23. Loss: 0.05760090425610542
Batch: 24. Loss: 0.05361459031701088
Batch: 25. Loss: 0.04881850630044937
Batch: 26. Loss: 0.1137707307934761
Batch: 27. Loss: 0.06306444108486176
Batch: 28. Loss: 0.10700549930334091
Batch: 29. Loss: 0.10431226342916489
Batch: 30. Loss: 0.06309416890144348
Batch: 31. Loss: 0.08589433878660202
Batch: 32. Loss: 0.09656716138124466
Batch: 33. Loss: 0.08861621469259262
Batch: 34. Loss: 0.09787002205848694
Batch: 35. Loss: 0.08559444546699524
Batch: 36. Loss: 0.06890618056058884
Batch: 37. Loss: 0.05905604735016823
-----------------------------------------
Epoch completed in 942.6269233226776 s.
Mean loss: 0.06755243241786957

Starting Epoch: 16/25, at: 1536531149.4331696
=========================================
Batch: 1. Loss: 0.10851314663887024
Batch: 2. Loss: 0.06338588893413544
Batch: 3. Loss: 0.14795543253421783
Batch: 4. Loss: 0.06130974367260933
Batch: 5. Loss: 0.07362048327922821
Batch: 6. Loss: 0.17504645884037018
Batch: 7. Loss: 0.09173162281513214
Batch: 8. Loss: 0.07808570563793182
Batch: 9. Loss: 0.08738207072019577
Batch: 10. Loss: 0.09347248077392578
Batch: 11. Loss: 0.08282402902841568
Batch: 12. Loss: 0.12979499995708466
Batch: 13. Loss: 0.11118927597999573
Batch: 14. Loss: 0.08009268343448639
Batch: 15. Loss: 0.09901342540979385
Batch: 16. Loss: 0.12139159440994263
Batch: 17. Loss: 0.08863700181245804
Batch: 18. Loss: 0.1072617918252945
Batch: 19. Loss: 0.24810661375522614
Batch: 20. Loss: 0.1261829286813736
Batch: 21. Loss: 0.1485861986875534
Batch: 22. Loss: 0.17760521173477173
Batch: 23. Loss: 0.1446039378643036
Batch: 24. Loss: 0.12769708037376404
Batch: 25. Loss: 0.11775748431682587
Batch: 26. Loss: 0.1726439744234085
Batch: 27. Loss: 0.15593095123767853
Batch: 28. Loss: 0.20114369690418243
Batch: 29. Loss: 0.12088320404291153
Batch: 30. Loss: 0.10664856433868408
Batch: 31. Loss: 0.08085140585899353
Batch: 32. Loss: 0.10659816861152649
Batch: 33. Loss: 0.11544064432382584
Batch: 34. Loss: 0.10207081586122513
Batch: 35. Loss: 0.10642364621162415
Batch: 36. Loss: 0.1005462035536766
Batch: 37. Loss: 0.08852014690637589
-----------------------------------------
Epoch completed in 940.0524134635925 s.
Mean loss: 0.11753915995359421

Starting Epoch: 17/25, at: 1536532089.4856164
=========================================
Batch: 1. Loss: 0.15780380368232727
Batch: 2. Loss: 0.14775648713111877
Batch: 3. Loss: 0.12277418375015259
Batch: 4. Loss: 0.10461553931236267
Batch: 5. Loss: 0.11108390986919403
Batch: 6. Loss: 0.0696345642209053
Batch: 7. Loss: 0.08267565816640854
Batch: 8. Loss: 0.13198436796665192
Batch: 9. Loss: 0.1000656709074974
Batch: 10. Loss: 0.09988714754581451
Batch: 11. Loss: 0.06930314749479294
Batch: 12. Loss: 0.06459705531597137
Batch: 13. Loss: 0.09644363075494766
Batch: 14. Loss: 0.1229611337184906
Batch: 15. Loss: 0.13700956106185913
Batch: 16. Loss: 0.08404367417097092
Batch: 17. Loss: 0.13926894962787628
Batch: 18. Loss: 0.09343963861465454
Batch: 19. Loss: 0.1008901298046112
Batch: 20. Loss: 0.07222753763198853
Batch: 21. Loss: 0.09883560240268707
Batch: 22. Loss: 0.09008157253265381
Batch: 23. Loss: 0.10224390029907227
Batch: 24. Loss: 0.11102605611085892
Batch: 25. Loss: 0.08980008959770203
Batch: 26. Loss: 0.08517467230558395
Batch: 27. Loss: 0.08278289437294006
Batch: 28. Loss: 0.08000540733337402
Batch: 29. Loss: 0.10214874893426895
Batch: 30. Loss: 0.10050397366285324
Batch: 31. Loss: 0.067894347012043
Batch: 32. Loss: 0.07616088539361954
Batch: 33. Loss: 0.07171095907688141
Batch: 34. Loss: 0.05825554579496384
Batch: 35. Loss: 0.07851696759462357
Batch: 36. Loss: 0.05778343230485916
Batch: 37. Loss: 0.10455042123794556
-----------------------------------------
Epoch completed in 933.1240291595459 s.
Mean loss: 0.09637678414583206

Starting Epoch: 18/25, at: 1536533022.609668
=========================================
Batch: 1. Loss: 0.0693284273147583
Batch: 2. Loss: 0.07513121515512466
Batch: 3. Loss: 0.07713703066110611
Batch: 4. Loss: 0.08129075914621353
Batch: 5. Loss: 0.07716260105371475
Batch: 6. Loss: 0.05700765550136566
Batch: 7. Loss: 0.06935218721628189
Batch: 8. Loss: 0.05445120483636856
Batch: 9. Loss: 0.05418575555086136
Batch: 10. Loss: 0.08288615196943283
Batch: 11. Loss: 0.06122441962361336
Batch: 12. Loss: 0.07246392965316772
Batch: 13. Loss: 0.06206667795777321
Batch: 14. Loss: 0.054037123918533325
Batch: 15. Loss: 0.07008058577775955
Batch: 16. Loss: 0.07838378101587296
Batch: 17. Loss: 0.06625780463218689
Batch: 18. Loss: 0.06841512024402618
Batch: 19. Loss: 0.05448543280363083
Batch: 20. Loss: 0.09850390255451202
Batch: 21. Loss: 0.06878267228603363
Batch: 22. Loss: 0.061181217432022095
Batch: 23. Loss: 0.07938707619905472
Batch: 24. Loss: 0.06391695141792297
Batch: 25. Loss: 0.07488459348678589
Batch: 26. Loss: 0.06419099867343903
Batch: 27. Loss: 0.06545593589544296
Batch: 28. Loss: 0.09871083498001099
Batch: 29. Loss: 0.08018089085817337
Batch: 30. Loss: 0.07085254043340683
Batch: 31. Loss: 0.05841723456978798
Batch: 32. Loss: 0.04767032712697983
Batch: 33. Loss: 0.04307316243648529
Batch: 34. Loss: 0.06084023416042328
Batch: 35. Loss: 0.07636694610118866
Batch: 36. Loss: 0.08671855181455612
Batch: 37. Loss: 0.13308636844158173
-----------------------------------------
Epoch completed in 934.9753632545471 s.
Mean loss: 0.070745088160038

Starting Epoch: 19/25, at: 1536533957.5850625
=========================================
Batch: 1. Loss: 0.07679535448551178
Batch: 2. Loss: 0.07801789045333862
Batch: 3. Loss: 0.06376026570796967
Batch: 4. Loss: 0.06797371059656143
Batch: 5. Loss: 0.07398606091737747
Batch: 6. Loss: 0.04744656756520271
Batch: 7. Loss: 0.07519032061100006
Batch: 8. Loss: 0.06922447681427002
Batch: 9. Loss: 0.06415549665689468
Batch: 10. Loss: 0.06849687546491623
Batch: 11. Loss: 0.0628080740571022
Batch: 12. Loss: 0.06111249327659607
Batch: 13. Loss: 0.08321996033191681
Batch: 14. Loss: 0.05968223512172699
Batch: 15. Loss: 0.04754510521888733
Batch: 16. Loss: 0.07402103394269943
Batch: 17. Loss: 0.08996566385030746
Batch: 18. Loss: 0.05202501639723778
Batch: 19. Loss: 0.06212291494011879
Batch: 20. Loss: 0.06786751747131348
Batch: 21. Loss: 0.05963767319917679
Batch: 22. Loss: 0.06333034485578537
Batch: 23. Loss: 0.07323842495679855
Batch: 24. Loss: 0.05751354992389679
Batch: 25. Loss: 0.050852540880441666
Batch: 26. Loss: 0.04347118362784386
Batch: 27. Loss: 0.08278248459100723
Batch: 28. Loss: 0.11535913497209549
Batch: 29. Loss: 0.07508446276187897
Batch: 30. Loss: 0.09125268459320068
Batch: 31. Loss: 0.07245849072933197
Batch: 32. Loss: 0.08657389879226685
Batch: 33. Loss: 0.06018051132559776
Batch: 34. Loss: 0.0695258378982544
Batch: 35. Loss: 0.09772083908319473
Batch: 36. Loss: 0.049215272068977356
Batch: 37. Loss: 0.0756942480802536
-----------------------------------------
Epoch completed in 943.5102941989899 s.
Mean loss: 0.06944077461957932

Starting Epoch: 20/25, at: 1536534901.0953746
=========================================
Batch: 1. Loss: 0.10786199569702148
Batch: 2. Loss: 0.07794461399316788
Batch: 3. Loss: 0.07430025935173035
Batch: 4. Loss: 0.059457503259181976
Batch: 5. Loss: 0.0776052251458168
Batch: 6. Loss: 0.06787890195846558
Batch: 7. Loss: 0.05076685547828674
Batch: 8. Loss: 0.053298573940992355
Batch: 9. Loss: 0.0628315880894661
Batch: 10. Loss: 0.04959767311811447
Batch: 11. Loss: 0.056410085409879684
Batch: 12. Loss: 0.06859336793422699
Batch: 13. Loss: 0.11909390985965729
Batch: 14. Loss: 0.10074315220117569
Batch: 15. Loss: 0.07155133783817291
Batch: 16. Loss: 0.08675756305456161
Batch: 17. Loss: 0.06465880572795868
Batch: 18. Loss: 0.06495337188243866
Batch: 19. Loss: 0.12152494490146637
Batch: 20. Loss: 0.058484725654125214
Batch: 21. Loss: 0.07192859798669815
Batch: 22. Loss: 0.1097043976187706
Batch: 23. Loss: 0.10375091433525085
Batch: 24. Loss: 0.06045886129140854
Batch: 25. Loss: 0.07083539664745331
Batch: 26. Loss: 0.07068556547164917
Batch: 27. Loss: 0.07085831463336945
Batch: 28. Loss: 0.0677548348903656
Batch: 29. Loss: 0.04745849594473839
Batch: 30. Loss: 0.0985444113612175
Batch: 31. Loss: 0.06134859099984169
Batch: 32. Loss: 0.06583122164011002
Batch: 33. Loss: 0.06587723642587662
Batch: 34. Loss: 0.054669808596372604
Batch: 35. Loss: 0.07586339116096497
Batch: 36. Loss: 0.07670918107032776
Batch: 37. Loss: 0.07292996346950531
-----------------------------------------
Epoch completed in 928.2753641605377 s.
Mean loss: 0.07404118031263351

Starting Epoch: 21/25, at: 1536535829.3707724
=========================================
Batch: 1. Loss: 0.07293955236673355
Batch: 2. Loss: 0.06253865361213684
Batch: 3. Loss: 0.04824596270918846
Batch: 4. Loss: 0.06469642370939255
Batch: 5. Loss: 0.07360316812992096
Batch: 6. Loss: 0.03594161942601204
Batch: 7. Loss: 0.05749080702662468
Batch: 8. Loss: 0.06428855657577515
Batch: 9. Loss: 0.04858829453587532
Batch: 10. Loss: 0.06988769769668579
Batch: 11. Loss: 0.06573586165904999
Batch: 12. Loss: 0.05549171194434166
Batch: 13. Loss: 0.059244554489851
Batch: 14. Loss: 0.04860881343483925
Batch: 15. Loss: 0.051801178604364395
Batch: 16. Loss: 0.05519243702292442
Batch: 17. Loss: 0.06878369301557541
Batch: 18. Loss: 0.0655115619301796
Batch: 19. Loss: 0.04158499836921692
Batch: 20. Loss: 0.04418807104229927
Batch: 21. Loss: 0.06756635755300522
Batch: 22. Loss: 0.07498425990343094
Batch: 23. Loss: 0.06479936093091965
Batch: 24. Loss: 0.06591599434614182
Batch: 25. Loss: 0.07424648106098175
Batch: 26. Loss: 0.0533362478017807
Batch: 27. Loss: 0.04480649530887604
Batch: 28. Loss: 0.05997774004936218
Batch: 29. Loss: 0.08343629539012909
Batch: 30. Loss: 0.05562848225235939
Batch: 31. Loss: 0.04153301566839218
Batch: 32. Loss: 0.04565338045358658
Batch: 33. Loss: 0.057646650820970535
Batch: 34. Loss: 0.05874216929078102
Batch: 35. Loss: 0.047832537442445755
Batch: 36. Loss: 0.03822169825434685
Batch: 37. Loss: 0.04271555319428444
-----------------------------------------
Epoch completed in 933.5562481880188 s.
Mean loss: 0.0576055683195591

Starting Epoch: 22/25, at: 1536536762.927045
=========================================
Batch: 1. Loss: 0.053603701293468475
Batch: 2. Loss: 0.058491773903369904
Batch: 3. Loss: 0.058222342282533646
Batch: 4. Loss: 0.08057624101638794
Batch: 5. Loss: 0.037661112844944
Batch: 6. Loss: 0.04817681014537811
Batch: 7. Loss: 0.04576805979013443
Batch: 8. Loss: 0.049776531755924225
Batch: 9. Loss: 0.04806409403681755
Batch: 10. Loss: 0.05893508344888687
Batch: 11. Loss: 0.0487116277217865
Batch: 12. Loss: 0.04631275683641434
Batch: 13. Loss: 0.04084508866071701
Batch: 14. Loss: 0.052755873650312424
Batch: 15. Loss: 0.03538620099425316
Batch: 16. Loss: 0.04416714236140251
Batch: 17. Loss: 0.03822982683777809
Batch: 18. Loss: 0.04138714075088501
Batch: 19. Loss: 0.07516256719827652
Batch: 20. Loss: 0.03077436424791813
Batch: 21. Loss: 0.04574410989880562
Batch: 22. Loss: 0.0597602017223835
Batch: 23. Loss: 0.046717360615730286
Batch: 24. Loss: 0.05405094847083092
Batch: 25. Loss: 0.048205580562353134
Batch: 26. Loss: 0.05190131440758705
Batch: 27. Loss: 0.05139869451522827
Batch: 28. Loss: 0.06577705591917038
Batch: 29. Loss: 0.032247282564640045
Batch: 30. Loss: 0.030976586043834686
Batch: 31. Loss: 0.06281484663486481
Batch: 32. Loss: 0.051001936197280884
Batch: 33. Loss: 0.03855184465646744
Batch: 34. Loss: 0.05307420715689659
Batch: 35. Loss: 0.04610737785696983
Batch: 36. Loss: 0.055714573711156845
Batch: 37. Loss: 0.08050249516963959
-----------------------------------------
Epoch completed in 931.5994925498962 s.
Mean loss: 0.05047445371747017

Starting Epoch: 23/25, at: 1536537694.5265625
=========================================
Batch: 1. Loss: 0.032775796949863434
Batch: 2. Loss: 0.055213332176208496
Batch: 3. Loss: 0.05771949887275696
Batch: 4. Loss: 0.0448782779276371
Batch: 5. Loss: 0.04540419951081276
Batch: 6. Loss: 0.04024096578359604
Batch: 7. Loss: 0.045747485011816025
Batch: 8. Loss: 0.03767484799027443
Batch: 9. Loss: 0.04890201985836029
Batch: 10. Loss: 0.04763248562812805
Batch: 11. Loss: 0.046881891787052155
Batch: 12. Loss: 0.05602916702628136
Batch: 13. Loss: 0.055584825575351715
Batch: 14. Loss: 0.05212480202317238
Batch: 15. Loss: 0.05471058562397957
Batch: 16. Loss: 0.05435185879468918
Batch: 17. Loss: 0.05664438381791115
Batch: 18. Loss: 0.04864701256155968
Batch: 19. Loss: 0.03961854428052902
Batch: 20. Loss: 0.040492646396160126
Batch: 21. Loss: 0.06603967398405075
Batch: 22. Loss: 0.0479615181684494
Batch: 23. Loss: 0.04790067672729492
Batch: 24. Loss: 0.06075622886419296
Batch: 25. Loss: 0.03968419134616852
Batch: 26. Loss: 0.04837151616811752
Batch: 27. Loss: 0.03463110700249672
Batch: 28. Loss: 0.047291822731494904
Batch: 29. Loss: 0.09151653945446014
Batch: 30. Loss: 0.04411301389336586
Batch: 31. Loss: 0.05400184914469719
Batch: 32. Loss: 0.05942927673459053
Batch: 33. Loss: 0.06526418775320053
Batch: 34. Loss: 0.049139298498630524
Batch: 35. Loss: 0.03863336518406868
Batch: 36. Loss: 0.045351020991802216
Batch: 37. Loss: 0.04588006064295769
-----------------------------------------
Epoch completed in 934.0606496334076 s.
Mean loss: 0.04992540553212166

Starting Epoch: 24/25, at: 1536538628.5872371
=========================================
Batch: 1. Loss: 0.06287124007940292
Batch: 2. Loss: 0.05018675699830055
Batch: 3. Loss: 0.05354900658130646
Batch: 4. Loss: 0.040239639580249786
Batch: 5. Loss: 0.07103943079710007
Batch: 6. Loss: 0.05421026051044464
Batch: 7. Loss: 0.037784211337566376
Batch: 8. Loss: 0.03886973485350609
Batch: 9. Loss: 0.05839499831199646
Batch: 10. Loss: 0.0480412095785141
Batch: 11. Loss: 0.046280115842819214
Batch: 12. Loss: 0.05856233835220337
Batch: 13. Loss: 0.028664767742156982
Batch: 14. Loss: 0.038761015981435776
Batch: 15. Loss: 0.02704082429409027
Batch: 16. Loss: 0.04295104742050171
Batch: 17. Loss: 0.03376763314008713
Batch: 18. Loss: 0.03349245712161064
Batch: 19. Loss: 0.056875549256801605
Batch: 20. Loss: 0.033225055783987045
Batch: 21. Loss: 0.051775671541690826
Batch: 22. Loss: 0.03248511999845505
Batch: 23. Loss: 0.04072194546461105
Batch: 24. Loss: 0.05034349486231804
Batch: 25. Loss: 0.03849032521247864
Batch: 26. Loss: 0.040731918066740036
Batch: 27. Loss: 0.051567066460847855
Batch: 28. Loss: 0.042708661407232285
Batch: 29. Loss: 0.045242875814437866
Batch: 30. Loss: 0.05074446275830269
Batch: 31. Loss: 0.03855954855680466
Batch: 32. Loss: 0.034614693373441696
Batch: 33. Loss: 0.034303929656744
Batch: 34. Loss: 0.04552276059985161
Batch: 35. Loss: 0.05673982948064804
Batch: 36. Loss: 0.048164211213588715
Batch: 37. Loss: 0.04966949671506882
-----------------------------------------
Epoch completed in 932.0568976402283 s.
Mean loss: 0.04505928233265877

Starting Epoch: 25/25, at: 1536539560.6441586
=========================================
Batch: 1. Loss: 0.04950915649533272
Batch: 2. Loss: 0.04441593587398529
Batch: 3. Loss: 0.05481859669089317
Batch: 4. Loss: 0.05128144845366478
Batch: 5. Loss: 0.03801460564136505
Batch: 6. Loss: 0.04545902460813522
Batch: 7. Loss: 0.043790992349386215
Batch: 8. Loss: 0.04006187245249748
Batch: 9. Loss: 0.04082106426358223
Batch: 10. Loss: 0.0322427824139595
Batch: 11. Loss: 0.03307051584124565
Batch: 12. Loss: 0.051339831203222275
Batch: 13. Loss: 0.07116739451885223
Batch: 14. Loss: 0.023600930348038673
Batch: 15. Loss: 0.04488253965973854
Batch: 16. Loss: 0.049022067338228226
Batch: 17. Loss: 0.04338221251964569
Batch: 18. Loss: 0.03429904207587242
Batch: 19. Loss: 0.04692218080163002
Batch: 20. Loss: 0.028021173551678658
Batch: 21. Loss: 0.040347665548324585
Batch: 22. Loss: 0.03837847337126732
Batch: 23. Loss: 0.044839970767498016
Batch: 24. Loss: 0.05268535017967224
Batch: 25. Loss: 0.04304245114326477
Batch: 26. Loss: 0.030363285914063454
Batch: 27. Loss: 0.04004473239183426
Batch: 28. Loss: 0.05409417673945427
Batch: 29. Loss: 0.03384609892964363
Batch: 30. Loss: 0.033296696841716766
Batch: 31. Loss: 0.05166035518050194
Batch: 32. Loss: 0.044461969286203384
Batch: 33. Loss: 0.03621022030711174
Batch: 34. Loss: 0.038920462131500244
Batch: 35. Loss: 0.041147634387016296
Batch: 36. Loss: 0.03014450892806053
Batch: 37. Loss: 0.016641288995742798
-----------------------------------------
Epoch completed in 926.6101577281952 s.
Mean loss: 0.04152023419737816

Finished training, saving the trained model
Training Finished. Saving test images to: ./runs/1536540499.7430353
