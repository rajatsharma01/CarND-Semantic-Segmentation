/home/rajat/miniconda3/envs/carnd-term1/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
TensorFlow Version: 1.10.0
main.py:18: UserWarning: No GPU found. Please use a GPU to train your neural network.
  warnings.warn('No GPU found. Please use a GPU to train your neural network.')
Tests Passed
Tests Passed
Tests Passed
Tests Passed
Tests Passed
Starting Epoch: 1, at: 1536334578.159814
=========================================
Batch: 1. Loss: 52.53948974609375
Batch: 2. Loss: 272.5469055175781
Batch: 3. Loss: 17.052156448364258
Batch: 4. Loss: 24.01150131225586
Batch: 5. Loss: 17.110809326171875
Batch: 6. Loss: 7.892388343811035
Batch: 7. Loss: 4.897597312927246
Batch: 8. Loss: 5.68956184387207
Batch: 9. Loss: 4.655947208404541
Batch: 10. Loss: 2.039268970489502
Batch: 11. Loss: 3.1731173992156982
Batch: 12. Loss: 3.4501760005950928
Batch: 13. Loss: 2.2950856685638428
Batch: 14. Loss: 1.0998104810714722
Batch: 15. Loss: 1.6959903240203857
-----------------------------------------
Epoch completed in 968.9836943149567 s.
Mean loss: 28.009986877441406

Starting Epoch: 2, at: 1536335547.1437118
=========================================
Batch: 1. Loss: 1.7433502674102783
Batch: 2. Loss: 1.3855572938919067
Batch: 3. Loss: 0.9611039757728577
Batch: 4. Loss: 0.9107617139816284
Batch: 5. Loss: 0.9159656167030334
Batch: 6. Loss: 0.9513700604438782
Batch: 7. Loss: 0.9409980773925781
Batch: 8. Loss: 0.8740476965904236
Batch: 9. Loss: 0.8125801086425781
Batch: 10. Loss: 0.7512720823287964
Batch: 11. Loss: 0.744770348072052
Batch: 12. Loss: 0.7667664289474487
Batch: 13. Loss: 0.7814883589744568
Batch: 14. Loss: 0.7835512161254883
Batch: 15. Loss: 0.7514346837997437
-----------------------------------------
Epoch completed in 923.7368531227112 s.
Mean loss: 0.9383345246315002

Starting Epoch: 3, at: 1536336470.880607
=========================================
Batch: 1. Loss: 0.7313212156295776
Batch: 2. Loss: 0.7043142318725586
Batch: 3. Loss: 0.7058490514755249
Batch: 4. Loss: 0.7042890191078186
Batch: 5. Loss: 0.7086172103881836
Batch: 6. Loss: 0.7060021758079529
Batch: 7. Loss: 0.703658938407898
Batch: 8. Loss: 0.701425313949585
Batch: 9. Loss: 0.6877288222312927
Batch: 10. Loss: 0.6831867098808289
Batch: 11. Loss: 0.6812427043914795
Batch: 12. Loss: 0.6807087659835815
Batch: 13. Loss: 0.6812896132469177
Batch: 14. Loss: 0.6793156862258911
Batch: 15. Loss: 0.6827867031097412
-----------------------------------------
Epoch completed in 916.597757101059 s.
Mean loss: 0.6961157321929932

Starting Epoch: 4, at: 1536337387.4785008
=========================================
Batch: 1. Loss: 0.6812899708747864
Batch: 2. Loss: 0.6760697960853577
Batch: 3. Loss: 0.6749952435493469
Batch: 4. Loss: 0.6699286103248596
Batch: 5. Loss: 0.6696688532829285
Batch: 6. Loss: 0.670404851436615
Batch: 7. Loss: 0.6685394048690796
Batch: 8. Loss: 0.6657202839851379
Batch: 9. Loss: 0.6684831380844116
Batch: 10. Loss: 0.6657472252845764
Batch: 11. Loss: 0.6702672839164734
Batch: 12. Loss: 0.6675751805305481
Batch: 13. Loss: 0.6645548343658447
Batch: 14. Loss: 0.6636836528778076
Batch: 15. Loss: 0.6657212972640991
-----------------------------------------
Epoch completed in 908.1598711013794 s.
Mean loss: 0.6695099472999573

Starting Epoch: 5, at: 1536338295.6384027
=========================================
Batch: 1. Loss: 0.6630557179450989
Batch: 2. Loss: 0.6629025340080261
Batch: 3. Loss: 0.6593520641326904
Batch: 4. Loss: 0.6597273945808411
Batch: 5. Loss: 0.6597480773925781
Batch: 6. Loss: 0.6563912630081177
Batch: 7. Loss: 0.6581673622131348
Batch: 8. Loss: 0.6537354588508606
Batch: 9. Loss: 0.6569687724113464
Batch: 10. Loss: 0.654522180557251
Batch: 11. Loss: 0.6554862856864929
Batch: 12. Loss: 0.655545711517334
Batch: 13. Loss: 0.6540275812149048
Batch: 14. Loss: 0.6525013446807861
Batch: 15. Loss: 0.6599389314651489
-----------------------------------------
Epoch completed in 909.9843902587891 s.
Mean loss: 0.6574713587760925

Starting Epoch: 6, at: 1536339205.6228213
=========================================
Batch: 1. Loss: 0.652110755443573
Batch: 2. Loss: 0.6494531631469727
Batch: 3. Loss: 0.6500116586685181
Batch: 4. Loss: 0.6500203609466553
Batch: 5. Loss: 0.6507471799850464
Batch: 6. Loss: 0.6450421214103699
Batch: 7. Loss: 0.6503399610519409
Batch: 8. Loss: 0.6484841704368591
Batch: 9. Loss: 0.6416418552398682
Batch: 10. Loss: 0.646091639995575
Batch: 11. Loss: 0.6416319608688354
Batch: 12. Loss: 0.6476083993911743
Batch: 13. Loss: 0.6451156735420227
Batch: 14. Loss: 0.6462932825088501
Batch: 15. Loss: 0.6460064649581909
-----------------------------------------
Epoch completed in 904.45663022995 s.
Mean loss: 0.6473731994628906

Starting Epoch: 7, at: 1536340110.079667
=========================================
Batch: 1. Loss: 0.641136884689331
Batch: 2. Loss: 0.6466212272644043
Batch: 3. Loss: 0.6362790465354919
Batch: 4. Loss: 0.6352697610855103
Batch: 5. Loss: 0.6374543309211731
Batch: 6. Loss: 0.6409744024276733
Batch: 7. Loss: 0.6387559771537781
Batch: 8. Loss: 0.6361649632453918
Batch: 9. Loss: 0.6395249962806702
Batch: 10. Loss: 0.6369790434837341
Batch: 11. Loss: 0.6352794170379639
Batch: 12. Loss: 0.635219395160675
Batch: 13. Loss: 0.6412029266357422
Batch: 14. Loss: 0.6366528272628784
Batch: 15. Loss: 0.6308863162994385
-----------------------------------------
Epoch completed in 910.5742995738983 s.
Mean loss: 0.6378934383392334

Starting Epoch: 8, at: 1536341020.653993
=========================================
Batch: 1. Loss: 0.6274287104606628
Batch: 2. Loss: 0.6317130923271179
Batch: 3. Loss: 0.6331300139427185
Batch: 4. Loss: 0.6288036704063416
Batch: 5. Loss: 0.6358660459518433
Batch: 6. Loss: 0.629294753074646
Batch: 7. Loss: 0.6311590075492859
Batch: 8. Loss: 0.6308494806289673
Batch: 9. Loss: 0.6284844279289246
Batch: 10. Loss: 0.6307461261749268
Batch: 11. Loss: 0.6230330467224121
Batch: 12. Loss: 0.6233336329460144
Batch: 13. Loss: 0.6297792792320251
Batch: 14. Loss: 0.6295549869537354
Batch: 15. Loss: 0.6185330152511597
-----------------------------------------
Epoch completed in 908.1627933979034 s.
Mean loss: 0.6287805438041687

Starting Epoch: 9, at: 1536341928.8168156
=========================================
Batch: 1. Loss: 0.6223745346069336
Batch: 2. Loss: 0.6245102882385254
Batch: 3. Loss: 0.6217595934867859
Batch: 4. Loss: 0.6262965202331543
Batch: 5. Loss: 0.6214945316314697
Batch: 6. Loss: 0.6234097480773926
Batch: 7. Loss: 0.6261951923370361
Batch: 8. Loss: 0.6215535402297974
Batch: 9. Loss: 0.6210777163505554
Batch: 10. Loss: 0.6207073330879211
Batch: 11. Loss: 0.6178027391433716
Batch: 12. Loss: 0.6137924790382385
Batch: 13. Loss: 0.6131212115287781
Batch: 14. Loss: 0.6141191720962524
Batch: 15. Loss: 0.6018556356430054
-----------------------------------------
Epoch completed in 920.4822919368744 s.
Mean loss: 0.6193379759788513

Starting Epoch: 10, at: 1536342849.2991364
=========================================
Batch: 1. Loss: 0.611530601978302
Batch: 2. Loss: 0.6107383966445923
Batch: 3. Loss: 0.6181034445762634
Batch: 4. Loss: 0.6134002208709717
Batch: 5. Loss: 0.6086943745613098
Batch: 6. Loss: 0.6075159311294556
Batch: 7. Loss: 0.6172066330909729
Batch: 8. Loss: 0.6136913299560547
Batch: 9. Loss: 0.6093828082084656
Batch: 10. Loss: 0.6056066155433655
Batch: 11. Loss: 0.611853837966919
Batch: 12. Loss: 0.604792058467865
Batch: 13. Loss: 0.6120972633361816
Batch: 14. Loss: 0.6049124598503113
Batch: 15. Loss: 0.607935905456543
-----------------------------------------
Epoch completed in 923.7087016105652 s.
Mean loss: 0.6104974746704102

Starting Epoch: 11, at: 1536343773.0078704
=========================================
Batch: 1. Loss: 0.6095162034034729
Batch: 2. Loss: 0.597263753414154
Batch: 3. Loss: 0.6051732301712036
Batch: 4. Loss: 0.602412760257721
Batch: 5. Loss: 0.6059885621070862
Batch: 6. Loss: 0.5965510606765747
Batch: 7. Loss: 0.6069455146789551
Batch: 8. Loss: 0.6011059284210205
Batch: 9. Loss: 0.5925447344779968
Batch: 10. Loss: 0.5949529409408569
Batch: 11. Loss: 0.6002016663551331
Batch: 12. Loss: 0.5950480699539185
Batch: 13. Loss: 0.5991833209991455
Batch: 14. Loss: 0.5973998308181763
Batch: 15. Loss: 0.597133994102478
-----------------------------------------
Epoch completed in 905.902049779892 s.
Mean loss: 0.600094735622406

Starting Epoch: 12, at: 1536344678.9099476
=========================================
Batch: 1. Loss: 0.6011348366737366
Batch: 2. Loss: 0.5839701294898987
Batch: 3. Loss: 0.5859494805335999
Batch: 4. Loss: 0.5950550436973572
Batch: 5. Loss: 0.594304084777832
Batch: 6. Loss: 0.5979697108268738
Batch: 7. Loss: 0.5914900898933411
Batch: 8. Loss: 0.582608699798584
Batch: 9. Loss: 0.5798131227493286
Batch: 10. Loss: 0.5893964171409607
Batch: 11. Loss: 0.5808433890342712
Batch: 12. Loss: 0.5802198052406311
Batch: 13. Loss: 0.5814704298973083
Batch: 14. Loss: 0.5839197039604187
Batch: 15. Loss: 0.5824025869369507
-----------------------------------------
Epoch completed in 904.6292314529419 s.
Mean loss: 0.5873697996139526

Starting Epoch: 13, at: 1536345583.5392716
=========================================
Batch: 1. Loss: 0.5766605734825134
Batch: 2. Loss: 0.5759410262107849
Batch: 3. Loss: 0.5676644444465637
Batch: 4. Loss: 0.5759313702583313
Batch: 5. Loss: 0.5658087730407715
Batch: 6. Loss: 0.5667739510536194
Batch: 7. Loss: 0.5747416019439697
Batch: 8. Loss: 0.570378303527832
Batch: 9. Loss: 0.5778534412384033
Batch: 10. Loss: 0.5717841386795044
Batch: 11. Loss: 0.5681020021438599
Batch: 12. Loss: 0.5715927481651306
Batch: 13. Loss: 0.5738565921783447
Batch: 14. Loss: 0.5582929849624634
Batch: 15. Loss: 0.5717560648918152
-----------------------------------------
Epoch completed in 912.6251287460327 s.
Mean loss: 0.5711424946784973

Starting Epoch: 14, at: 1536346496.1645079
=========================================
Batch: 1. Loss: 0.5621545910835266
Batch: 2. Loss: 0.561735987663269
Batch: 3. Loss: 0.5483786463737488
Batch: 4. Loss: 0.5607632398605347
Batch: 5. Loss: 0.5553403496742249
Batch: 6. Loss: 0.5576707124710083
Batch: 7. Loss: 0.5480663776397705
Batch: 8. Loss: 0.5505163669586182
Batch: 9. Loss: 0.5372998118400574
Batch: 10. Loss: 0.5463517904281616
Batch: 11. Loss: 0.5415008664131165
Batch: 12. Loss: 0.5490221381187439
Batch: 13. Loss: 0.5405449867248535
Batch: 14. Loss: 0.54514479637146
Batch: 15. Loss: 0.5699758529663086
-----------------------------------------
Epoch completed in 915.8977072238922 s.
Mean loss: 0.5516310930252075

Starting Epoch: 15, at: 1536347412.062375
=========================================
Batch: 1. Loss: 0.5438478589057922
Batch: 2. Loss: 0.5324004888534546
Batch: 3. Loss: 0.5313321352005005
Batch: 4. Loss: 0.5430655479431152
Batch: 5. Loss: 0.5319099426269531
Batch: 6. Loss: 0.5343406200408936
Batch: 7. Loss: 0.5297877788543701
Batch: 8. Loss: 0.5201523900032043
Batch: 9. Loss: 0.514214277267456
Batch: 10. Loss: 0.5283044576644897
Batch: 11. Loss: 0.5183187127113342
Batch: 12. Loss: 0.5061418414115906
Batch: 13. Loss: 0.5154136419296265
Batch: 14. Loss: 0.5164918303489685
Batch: 15. Loss: 0.5031339526176453
-----------------------------------------
Epoch completed in 902.1117036342621 s.
Mean loss: 0.5245903730392456

Starting Epoch: 16, at: 1536348314.1741097
=========================================
Batch: 1. Loss: 0.5116986632347107
Batch: 2. Loss: 0.5030602812767029
Batch: 3. Loss: 0.4993335008621216
Batch: 4. Loss: 0.48654797673225403
Batch: 5. Loss: 0.505851686000824
Batch: 6. Loss: 0.49405747652053833
Batch: 7. Loss: 0.4882824122905731
Batch: 8. Loss: 0.49585622549057007
Batch: 9. Loss: 0.4830608367919922
Batch: 10. Loss: 0.4954325556755066
Batch: 11. Loss: 0.4962700605392456
Batch: 12. Loss: 0.4753761887550354
Batch: 13. Loss: 0.48370084166526794
Batch: 14. Loss: 0.4734295606613159
Batch: 15. Loss: 0.45732006430625916
-----------------------------------------
Epoch completed in 904.4268805980682 s.
Mean loss: 0.4899519085884094

Starting Epoch: 17, at: 1536349218.601024
=========================================
Batch: 1. Loss: 0.47289496660232544
Batch: 2. Loss: 0.46203693747520447
Batch: 3. Loss: 0.46062785387039185
Batch: 4. Loss: 0.4678495526313782
Batch: 5. Loss: 0.4506649971008301
Batch: 6. Loss: 0.4678873121738434
Batch: 7. Loss: 0.4556366503238678
Batch: 8. Loss: 0.44567447900772095
Batch: 9. Loss: 0.43076565861701965
Batch: 10. Loss: 0.4315860867500305
Batch: 11. Loss: 0.44879987835884094
Batch: 12. Loss: 0.4440535604953766
Batch: 13. Loss: 0.42728766798973083
Batch: 14. Loss: 0.4202825725078583
Batch: 15. Loss: 0.44814634323120117
-----------------------------------------
Epoch completed in 911.8560411930084 s.
Mean loss: 0.4489463269710541

Starting Epoch: 18, at: 1536350130.4570856
=========================================
Batch: 1. Loss: 0.42156660556793213
Batch: 2. Loss: 0.42016348242759705
Batch: 3. Loss: 0.42064040899276733
Batch: 4. Loss: 0.40783926844596863
Batch: 5. Loss: 0.40634825825691223
Batch: 6. Loss: 0.40868669748306274
Batch: 7. Loss: 0.4020065665245056
Batch: 8. Loss: 0.3977384567260742
Batch: 9. Loss: 0.3808785080909729
Batch: 10. Loss: 0.3804610073566437
Batch: 11. Loss: 0.399710476398468
Batch: 12. Loss: 0.3771013617515564
Batch: 13. Loss: 0.36810725927352905
Batch: 14. Loss: 0.36146098375320435
Batch: 15. Loss: 0.35957998037338257
-----------------------------------------
Epoch completed in 909.049886226654 s.
Mean loss: 0.3941526710987091

Starting Epoch: 19, at: 1536351039.5069962
=========================================
Batch: 1. Loss: 0.38665884733200073
Batch: 2. Loss: 0.3560216724872589
Batch: 3. Loss: 0.37257081270217896
Batch: 4. Loss: 0.35353121161460876
Batch: 5. Loss: 0.3684442341327667
Batch: 6. Loss: 0.3599036931991577
Batch: 7. Loss: 0.31962850689888
Batch: 8. Loss: 0.3507186770439148
Batch: 9. Loss: 0.33501192927360535
Batch: 10. Loss: 0.3283695578575134
Batch: 11. Loss: 0.3532046973705292
Batch: 12. Loss: 0.3125920295715332
Batch: 13. Loss: 0.329785019159317
Batch: 14. Loss: 0.33896827697753906
Batch: 15. Loss: 0.30621859431266785
-----------------------------------------
Epoch completed in 910.9542653560638 s.
Mean loss: 0.3447751998901367

Starting Epoch: 20, at: 1536351950.461294
=========================================
Batch: 1. Loss: 0.3113155663013458
Batch: 2. Loss: 0.3038502335548401
Batch: 3. Loss: 0.3258157968521118
Batch: 4. Loss: 0.30930691957473755
Batch: 5. Loss: 0.2762499749660492
Batch: 6. Loss: 0.31516996026039124
Batch: 7. Loss: 0.31522995233535767
Batch: 8. Loss: 0.33617594838142395
Batch: 9. Loss: 0.3278158903121948
Batch: 10. Loss: 0.26568350195884705
Batch: 11. Loss: 0.29333677887916565
Batch: 12. Loss: 0.31173625588417053
Batch: 13. Loss: 0.2812342941761017
Batch: 14. Loss: 0.30235907435417175
Batch: 15. Loss: 0.2842191159725189
-----------------------------------------
Epoch completed in 913.4591121673584 s.
Mean loss: 0.3039666414260864

Starting Epoch: 21, at: 1536352863.9204326
=========================================
Batch: 1. Loss: 0.2627607583999634
Batch: 2. Loss: 0.2921523153781891
Batch: 3. Loss: 0.2631184458732605
Batch: 4. Loss: 0.26491478085517883
Batch: 5. Loss: 0.2791290581226349
Batch: 6. Loss: 0.2481948882341385
Batch: 7. Loss: 0.2463965117931366
Batch: 8. Loss: 0.27019450068473816
Batch: 9. Loss: 0.26702675223350525
Batch: 10. Loss: 0.2867916524410248
Batch: 11. Loss: 0.24138423800468445
Batch: 12. Loss: 0.24973958730697632
Batch: 13. Loss: 0.26423606276512146
Batch: 14. Loss: 0.2832566499710083
Batch: 15. Loss: 0.23318293690681458
-----------------------------------------
Epoch completed in 914.0381877422333 s.
Mean loss: 0.26349860429763794

Starting Epoch: 22, at: 1536353777.9586544
=========================================
Batch: 1. Loss: 0.26255306601524353
Batch: 2. Loss: 0.2304951697587967
Batch: 3. Loss: 0.2527541518211365
Batch: 4. Loss: 0.2519943118095398
Batch: 5. Loss: 0.2643984854221344
Batch: 6. Loss: 0.24249054491519928
Batch: 7. Loss: 0.250523179769516
Batch: 8. Loss: 0.25630950927734375
Batch: 9. Loss: 0.2541191279888153
Batch: 10. Loss: 0.2650550603866577
Batch: 11. Loss: 0.234137162566185
Batch: 12. Loss: 0.23954494297504425
Batch: 13. Loss: 0.2542533278465271
Batch: 14. Loss: 0.2622582018375397
Batch: 15. Loss: 0.26857873797416687
-----------------------------------------
Epoch completed in 909.6019814014435 s.
Mean loss: 0.2526310086250305

Starting Epoch: 23, at: 1536354687.5606644
=========================================
Batch: 1. Loss: 0.2552800178527832
Batch: 2. Loss: 0.24157938361167908
Batch: 3. Loss: 0.2498038113117218
Batch: 4. Loss: 0.25689050555229187
Batch: 5. Loss: 0.23822379112243652
Batch: 6. Loss: 0.2405991554260254
Batch: 7. Loss: 0.22095844149589539
Batch: 8. Loss: 0.25343576073646545
Batch: 9. Loss: 0.24028344452381134
Batch: 10. Loss: 0.2287156581878662
Batch: 11. Loss: 0.25533920526504517
Batch: 12. Loss: 0.21213941276073456
Batch: 13. Loss: 0.203652486205101
Batch: 14. Loss: 0.24071532487869263
Batch: 15. Loss: 0.2066647708415985
-----------------------------------------
Epoch completed in 909.6825523376465 s.
Mean loss: 0.23628540337085724

Starting Epoch: 24, at: 1536355597.243252
=========================================
Batch: 1. Loss: 0.22264370322227478
Batch: 2. Loss: 0.23720282316207886
Batch: 3. Loss: 0.22345635294914246
Batch: 4. Loss: 0.2086888700723648
Batch: 5. Loss: 0.21601402759552002
Batch: 6. Loss: 0.22868818044662476
Batch: 7. Loss: 0.24253712594509125
Batch: 8. Loss: 0.21356186270713806
Batch: 9. Loss: 0.2647344768047333
Batch: 10. Loss: 0.22216427326202393
Batch: 11. Loss: 0.2522119879722595
Batch: 12. Loss: 0.23402926325798035
Batch: 13. Loss: 0.22269006073474884
Batch: 14. Loss: 0.22002556920051575
Batch: 15. Loss: 0.22812187671661377
-----------------------------------------
Epoch completed in 909.7698092460632 s.
Mean loss: 0.22911803424358368

Starting Epoch: 25, at: 1536356507.013206
=========================================
Batch: 1. Loss: 0.20630189776420593
Batch: 2. Loss: 0.22198499739170074
Batch: 3. Loss: 0.24167335033416748
Batch: 4. Loss: 0.22949101030826569
Batch: 5. Loss: 0.2316935509443283
Batch: 6. Loss: 0.23736654222011566
Batch: 7. Loss: 0.2019563466310501
Batch: 8. Loss: 0.2126482129096985
Batch: 9. Loss: 0.22715604305267334
Batch: 10. Loss: 0.21903975307941437
Batch: 11. Loss: 0.21573379635810852
Batch: 12. Loss: 0.24081182479858398
Batch: 13. Loss: 0.1940978765487671
Batch: 14. Loss: 0.21275804936885834
Batch: 15. Loss: 0.23900669813156128
-----------------------------------------
Epoch completed in 917.4134967327118 s.
Mean loss: 0.2221146672964096

Starting Epoch: 26, at: 1536357424.426732
=========================================
Batch: 1. Loss: 0.21533237397670746
Batch: 2. Loss: 0.23186500370502472
Batch: 3. Loss: 0.23883680999279022
Batch: 4. Loss: 0.21241435408592224
Batch: 5. Loss: 0.21860770881175995
Batch: 6. Loss: 0.21391849219799042
Batch: 7. Loss: 0.20963741838932037
Batch: 8. Loss: 0.2109736055135727
Batch: 9. Loss: 0.2027718424797058
Batch: 10. Loss: 0.19846615195274353
Batch: 11. Loss: 0.22575709223747253
Batch: 12. Loss: 0.20037081837654114
Batch: 13. Loss: 0.21932746469974518
Batch: 14. Loss: 0.1942785084247589
Batch: 15. Loss: 0.21731607615947723
-----------------------------------------
Epoch completed in 910.5710136890411 s.
Mean loss: 0.21399158239364624

Starting Epoch: 27, at: 1536358334.9977815
=========================================
Batch: 1. Loss: 0.18830905854701996
Batch: 2. Loss: 0.205671489238739
Batch: 3. Loss: 0.191287100315094
Batch: 4. Loss: 0.2080562710762024
Batch: 5. Loss: 0.23823773860931396
Batch: 6. Loss: 0.21252845227718353
Batch: 7. Loss: 0.21331749856472015
Batch: 8. Loss: 0.19384697079658508
Batch: 9. Loss: 0.18851056694984436
Batch: 10. Loss: 0.18441931903362274
Batch: 11. Loss: 0.20259514451026917
Batch: 12. Loss: 0.21534404158592224
Batch: 13. Loss: 0.19581301510334015
Batch: 14. Loss: 0.19456158578395844
Batch: 15. Loss: 0.20307087898254395
-----------------------------------------
Epoch completed in 909.6182758808136 s.
Mean loss: 0.2023712694644928

Starting Epoch: 28, at: 1536359244.6160836
=========================================
Batch: 1. Loss: 0.18789544701576233
Batch: 2. Loss: 0.18612200021743774
Batch: 3. Loss: 0.19506120681762695
Batch: 4. Loss: 0.18513721227645874
Batch: 5. Loss: 0.23386335372924805
Batch: 6. Loss: 0.20888400077819824
Batch: 7. Loss: 0.18720072507858276
Batch: 8. Loss: 0.22273646295070648
Batch: 9. Loss: 0.19063632190227509
Batch: 10. Loss: 0.21373611688613892
Batch: 11. Loss: 0.22627335786819458
Batch: 12. Loss: 0.2295333594083786
Batch: 13. Loss: 0.19955356419086456
Batch: 14. Loss: 0.18202081322669983
Batch: 15. Loss: 0.16869965195655823
-----------------------------------------
Epoch completed in 913.579407453537 s.
Mean loss: 0.20115691423416138

Starting Epoch: 29, at: 1536360158.195516
=========================================
Batch: 1. Loss: 0.1999461054801941
Batch: 2. Loss: 0.23168711364269257
Batch: 3. Loss: 0.21220585703849792
Batch: 4. Loss: 0.20071597397327423
Batch: 5. Loss: 0.19789671897888184
Batch: 6. Loss: 0.2094208300113678
Batch: 7. Loss: 0.2048693150281906
Batch: 8. Loss: 0.2136518359184265
Batch: 9. Loss: 0.2043750286102295
Batch: 10. Loss: 0.17632514238357544
Batch: 11. Loss: 0.20221994817256927
Batch: 12. Loss: 0.1909165382385254
Batch: 13. Loss: 0.20356044173240662
Batch: 14. Loss: 0.26432228088378906
Batch: 15. Loss: 0.17235970497131348
-----------------------------------------
Epoch completed in 913.1133162975311 s.
Mean loss: 0.20563150942325592

Starting Epoch: 30, at: 1536361071.308871
=========================================
Batch: 1. Loss: 0.19753126800060272
Batch: 2. Loss: 0.23563118278980255
Batch: 3. Loss: 0.2157503217458725
Batch: 4. Loss: 0.23510505259037018
Batch: 5. Loss: 0.1801213175058365
Batch: 6. Loss: 0.18089362978935242
Batch: 7. Loss: 0.18363040685653687
Batch: 8. Loss: 0.178033247590065
Batch: 9. Loss: 0.1960357278585434
Batch: 10. Loss: 0.1819884032011032
Batch: 11. Loss: 0.21133540570735931
Batch: 12. Loss: 0.20676513016223907
Batch: 13. Loss: 0.2097053825855255
Batch: 14. Loss: 0.18421928584575653
Batch: 15. Loss: 0.21338032186031342
-----------------------------------------
Epoch completed in 917.0307195186615 s.
Mean loss: 0.20067507028579712

Starting Epoch: 31, at: 1536361988.3396263
=========================================
Batch: 1. Loss: 0.1884433627128601
Batch: 2. Loss: 0.19753648340702057
Batch: 3. Loss: 0.19097350537776947
Batch: 4. Loss: 0.2397148609161377
Batch: 5. Loss: 0.18860748410224915
Batch: 6. Loss: 0.17762702703475952
Batch: 7. Loss: 0.18854112923145294
Batch: 8. Loss: 0.1950150728225708
Batch: 9. Loss: 0.17891716957092285
Batch: 10. Loss: 0.20183919370174408
Batch: 11. Loss: 0.18117722868919373
Batch: 12. Loss: 0.20184014737606049
Batch: 13. Loss: 0.19770361483097076
Batch: 14. Loss: 0.16323505342006683
Batch: 15. Loss: 0.20168744027614594
-----------------------------------------
Epoch completed in 916.2333464622498 s.
Mean loss: 0.19285723567008972

Starting Epoch: 32, at: 1536362904.5730004
=========================================
Batch: 1. Loss: 0.18643364310264587
Batch: 2. Loss: 0.17362965643405914
Batch: 3. Loss: 0.20844462513923645
Batch: 4. Loss: 0.1632288545370102
Batch: 5. Loss: 0.1761728972196579
Batch: 6. Loss: 0.1791735142469406
Batch: 7. Loss: 0.18167023360729218
Batch: 8. Loss: 0.21436645090579987
Batch: 9. Loss: 0.1663212776184082
Batch: 10. Loss: 0.18188461661338806
Batch: 11. Loss: 0.17249932885169983
Batch: 12. Loss: 0.17479297518730164
Batch: 13. Loss: 0.1839185208082199
Batch: 14. Loss: 0.17872019112110138
Batch: 15. Loss: 0.19963981211185455
-----------------------------------------
Epoch completed in 914.700042963028 s.
Mean loss: 0.1827264428138733

Starting Epoch: 33, at: 1536363819.2730815
=========================================
Batch: 1. Loss: 0.16184484958648682
Batch: 2. Loss: 0.19080118834972382
Batch: 3. Loss: 0.18770675361156464
Batch: 4. Loss: 0.16889667510986328
Batch: 5. Loss: 0.17693735659122467
Batch: 6. Loss: 0.14780831336975098
Batch: 7. Loss: 0.1705797016620636
Batch: 8. Loss: 0.18520329892635345
Batch: 9. Loss: 0.18065094947814941
Batch: 10. Loss: 0.21621786057949066
Batch: 11. Loss: 0.2200901359319687
Batch: 12. Loss: 0.18142150342464447
Batch: 13. Loss: 0.25479382276535034
Batch: 14. Loss: 0.17512452602386475
Batch: 15. Loss: 0.19762030243873596
-----------------------------------------
Epoch completed in 925.9318809509277 s.
Mean loss: 0.1877131462097168

Starting Epoch: 34, at: 1536364745.204996
=========================================
Batch: 1. Loss: 0.21552954614162445
Batch: 2. Loss: 0.20236706733703613
Batch: 3. Loss: 0.18320924043655396
Batch: 4. Loss: 0.18233604729175568
Batch: 5. Loss: 0.19301791489124298
Batch: 6. Loss: 0.19241221249103546
Batch: 7. Loss: 0.21162672340869904
Batch: 8. Loss: 0.18856051564216614
Batch: 9. Loss: 0.19160132110118866
Batch: 10. Loss: 0.20469407737255096
Batch: 11. Loss: 0.19364531338214874
Batch: 12. Loss: 0.18545877933502197
Batch: 13. Loss: 0.16776546835899353
Batch: 14. Loss: 0.16267453134059906
Batch: 15. Loss: 0.16819743812084198
-----------------------------------------
Epoch completed in 908.9648239612579 s.
Mean loss: 0.18953971564769745

Starting Epoch: 35, at: 1536365654.1699128
=========================================
Batch: 1. Loss: 0.17656807601451874
Batch: 2. Loss: 0.1516731083393097
Batch: 3. Loss: 0.21400612592697144
Batch: 4. Loss: 0.18807365000247955
Batch: 5. Loss: 0.18274986743927002
Batch: 6. Loss: 0.1646004617214203
Batch: 7. Loss: 0.16362342238426208
Batch: 8. Loss: 0.20867587625980377
Batch: 9. Loss: 0.15235480666160583
Batch: 10. Loss: 0.15787126123905182
Batch: 11. Loss: 0.20019391179084778
Batch: 12. Loss: 0.1716780811548233
Batch: 13. Loss: 0.17402812838554382
Batch: 14. Loss: 0.16825194656848907
Batch: 15. Loss: 0.15418705344200134
-----------------------------------------
Epoch completed in 911.9037053585052 s.
Mean loss: 0.17523571848869324

Starting Epoch: 36, at: 1536366566.0736403
=========================================
Batch: 1. Loss: 0.16737830638885498
Batch: 2. Loss: 0.17262665927410126
Batch: 3. Loss: 0.18446271121501923
Batch: 4. Loss: 0.1888798326253891
Batch: 5. Loss: 0.16308851540088654
Batch: 6. Loss: 0.19112101197242737
Batch: 7. Loss: 0.15478895604610443
Batch: 8. Loss: 0.15637081861495972
Batch: 9. Loss: 0.17256620526313782
Batch: 10. Loss: 0.19853484630584717
Batch: 11. Loss: 0.1584300398826599
Batch: 12. Loss: 0.15399841964244843
Batch: 13. Loss: 0.1589275449514389
Batch: 14. Loss: 0.19073909521102905
Batch: 15. Loss: 0.19001223146915436
-----------------------------------------
Epoch completed in 916.910667181015 s.
Mean loss: 0.1734616756439209

Starting Epoch: 37, at: 1536367482.9843357
=========================================
Batch: 1. Loss: 0.19732661545276642
Batch: 2. Loss: 0.161712646484375
Batch: 3. Loss: 0.17053073644638062
Batch: 4. Loss: 0.1628895401954651
Batch: 5. Loss: 0.18224453926086426
Batch: 6. Loss: 0.16945785284042358
Batch: 7. Loss: 0.1870202273130417
Batch: 8. Loss: 0.165780708193779
Batch: 9. Loss: 0.2034837156534195
Batch: 10. Loss: 0.15692515671253204
Batch: 11. Loss: 0.15320779383182526
Batch: 12. Loss: 0.1759118288755417
Batch: 13. Loss: 0.17420493066310883
Batch: 14. Loss: 0.1699259728193283
Batch: 15. Loss: 0.13701972365379333
-----------------------------------------
Epoch completed in 919.377459526062 s.
Mean loss: 0.17117612063884735

Starting Epoch: 38, at: 1536368402.3618927
=========================================
Batch: 1. Loss: 0.14998053014278412
Batch: 2. Loss: 0.17356789112091064
Batch: 3. Loss: 0.14535818994045258
Batch: 4. Loss: 0.18150223791599274
Batch: 5. Loss: 0.15643934905529022
Batch: 6. Loss: 0.18685349822044373
Batch: 7. Loss: 0.16625627875328064
Batch: 8. Loss: 0.15415704250335693
Batch: 9. Loss: 0.19512756168842316
Batch: 10. Loss: 0.1865547001361847
Batch: 11. Loss: 0.19873158633708954
Batch: 12. Loss: 0.17556209862232208
Batch: 13. Loss: 0.1437828540802002
Batch: 14. Loss: 0.18484428524971008
Batch: 15. Loss: 0.1478826105594635
-----------------------------------------
Epoch completed in 909.6698095798492 s.
Mean loss: 0.1697733849287033

Starting Epoch: 39, at: 1536369312.031732
=========================================
Batch: 1. Loss: 0.17217010259628296
Batch: 2. Loss: 0.1911538541316986
Batch: 3. Loss: 0.14486411213874817
Batch: 4. Loss: 0.1711733192205429
Batch: 5. Loss: 0.1738208383321762
Batch: 6. Loss: 0.1509021371603012
Batch: 7. Loss: 0.15167270600795746
Batch: 8. Loss: 0.17315945029258728
Batch: 9. Loss: 0.1805686205625534
Batch: 10. Loss: 0.15039530396461487
Batch: 11. Loss: 0.17314501106739044
Batch: 12. Loss: 0.18125846982002258
Batch: 13. Loss: 0.15858642756938934
Batch: 14. Loss: 0.18868885934352875
Batch: 15. Loss: 0.17321427166461945
-----------------------------------------
Epoch completed in 912.892560005188 s.
Mean loss: 0.16898488998413086

Starting Epoch: 40, at: 1536370224.9243293
=========================================
Batch: 1. Loss: 0.1729375272989273
Batch: 2. Loss: 0.1719798743724823
Batch: 3. Loss: 0.16484157741069794
Batch: 4. Loss: 0.16374057531356812
Batch: 5. Loss: 0.22405192255973816
Batch: 6. Loss: 0.16109158098697662
Batch: 7. Loss: 0.18342895805835724
Batch: 8. Loss: 0.16918811202049255
Batch: 9. Loss: 0.15728168189525604
Batch: 10. Loss: 0.1972072869539261
Batch: 11. Loss: 0.17975172400474548
Batch: 12. Loss: 0.15298312902450562
Batch: 13. Loss: 0.15940682590007782
Batch: 14. Loss: 0.19585976004600525
Batch: 15. Loss: 0.17088569700717926
-----------------------------------------
Epoch completed in 918.0662477016449 s.
Mean loss: 0.17497576773166656

Starting Epoch: 41, at: 1536371142.9906073
=========================================
Batch: 1. Loss: 0.15269945561885834
Batch: 2. Loss: 0.1745765507221222
Batch: 3. Loss: 0.158236563205719
Batch: 4. Loss: 0.17113490402698517
Batch: 5. Loss: 0.17497597634792328
Batch: 6. Loss: 0.14981010556221008
Batch: 7. Loss: 0.16083300113677979
Batch: 8. Loss: 0.16732625663280487
Batch: 9. Loss: 0.18656833469867706
Batch: 10. Loss: 0.15600983798503876
Batch: 11. Loss: 0.1578371524810791
Batch: 12. Loss: 0.18302622437477112
Batch: 13. Loss: 0.16174167394638062
Batch: 14. Loss: 0.17298197746276855
Batch: 15. Loss: 0.1484239250421524
-----------------------------------------
Epoch completed in 917.4418776035309 s.
Mean loss: 0.1650787889957428

Starting Epoch: 42, at: 1536372060.4325273
=========================================
Batch: 1. Loss: 0.18019543588161469
Batch: 2. Loss: 0.14005690813064575
Batch: 3. Loss: 0.155067577958107
Batch: 4. Loss: 0.16135980188846588
Batch: 5. Loss: 0.15443949401378632
Batch: 6. Loss: 0.16411976516246796
Batch: 7. Loss: 0.1642758995294571
Batch: 8. Loss: 0.14466722309589386
Batch: 9. Loss: 0.13765041530132294
Batch: 10. Loss: 0.15703710913658142
Batch: 11. Loss: 0.15398171544075012
Batch: 12. Loss: 0.14128459990024567
Batch: 13. Loss: 0.16251251101493835
Batch: 14. Loss: 0.15022243559360504
Batch: 15. Loss: 0.21866032481193542
-----------------------------------------
Epoch completed in 948.3834936618805 s.
Mean loss: 0.15903541445732117

Starting Epoch: 43, at: 1536373008.8160977
=========================================
Batch: 1. Loss: 0.14793239533901215
Batch: 2. Loss: 0.18792662024497986
Batch: 3. Loss: 0.1752529889345169
Batch: 4. Loss: 0.1489940732717514
Batch: 5. Loss: 0.1690211147069931
Batch: 6. Loss: 0.17143666744232178
Batch: 7. Loss: 0.1645357459783554
Batch: 8. Loss: 0.2099670022726059
Batch: 9. Loss: 0.19536416232585907
Batch: 10. Loss: 0.18395599722862244
Batch: 11. Loss: 0.14979435503482819
Batch: 12. Loss: 0.142549067735672
Batch: 13. Loss: 0.1628960222005844
Batch: 14. Loss: 0.17589813470840454
Batch: 15. Loss: 0.1444457769393921
-----------------------------------------
Epoch completed in 909.4418396949768 s.
Mean loss: 0.16866467893123627

Starting Epoch: 44, at: 1536373918.2579713
=========================================
Batch: 1. Loss: 0.1958257406949997
Batch: 2. Loss: 0.15440569818019867
Batch: 3. Loss: 0.1701461523771286
Batch: 4. Loss: 0.14762690663337708
Batch: 5. Loss: 0.1522350013256073
Batch: 6. Loss: 0.14727874100208282
Batch: 7. Loss: 0.16276037693023682
Batch: 8. Loss: 0.15560483932495117
Batch: 9. Loss: 0.1568128913640976
Batch: 10. Loss: 0.1842307597398758
Batch: 11. Loss: 0.14138945937156677
Batch: 12. Loss: 0.17471374571323395
Batch: 13. Loss: 0.16602003574371338
Batch: 14. Loss: 0.18046532571315765
Batch: 15. Loss: 0.16274191439151764
-----------------------------------------
Epoch completed in 916.5713326931 s.
Mean loss: 0.16348382830619812

Starting Epoch: 45, at: 1536374834.829371
=========================================
Batch: 1. Loss: 0.20456410944461823
Batch: 2. Loss: 0.16550213098526
Batch: 3. Loss: 0.14664097130298615
Batch: 4. Loss: 0.18864908814430237
Batch: 5. Loss: 0.15185347199440002
Batch: 6. Loss: 0.17691552639007568
Batch: 7. Loss: 0.15976494550704956
Batch: 8. Loss: 0.1541728377342224
Batch: 9. Loss: 0.13535921275615692
Batch: 10. Loss: 0.14661891758441925
Batch: 11. Loss: 0.13233798742294312
Batch: 12. Loss: 0.1611912101507187
Batch: 13. Loss: 0.16513366997241974
Batch: 14. Loss: 0.16119951009750366
Batch: 15. Loss: 0.12952788174152374
-----------------------------------------
Epoch completed in 914.2666268348694 s.
Mean loss: 0.15862876176834106

Starting Epoch: 46, at: 1536375749.0960279
=========================================
Batch: 1. Loss: 0.15561886131763458
Batch: 2. Loss: 0.15578125417232513
Batch: 3. Loss: 0.1430855244398117
Batch: 4. Loss: 0.17324818670749664
Batch: 5. Loss: 0.14571407437324524
Batch: 6. Loss: 0.13633747398853302
Batch: 7. Loss: 0.13721433281898499
Batch: 8. Loss: 0.156637504696846
Batch: 9. Loss: 0.16008175909519196
Batch: 10. Loss: 0.15961158275604248
Batch: 11. Loss: 0.12148252874612808
Batch: 12. Loss: 0.1681879311800003
Batch: 13. Loss: 0.14915578067302704
Batch: 14. Loss: 0.1533271223306656
Batch: 15. Loss: 0.13047732412815094
-----------------------------------------
Epoch completed in 918.0217776298523 s.
Mean loss: 0.14973075687885284

Starting Epoch: 47, at: 1536376667.117835
=========================================
Batch: 1. Loss: 0.16121971607208252
Batch: 2. Loss: 0.1500953882932663
Batch: 3. Loss: 0.14400145411491394
Batch: 4. Loss: 0.15523093938827515
Batch: 5. Loss: 0.12890949845314026
Batch: 6. Loss: 0.1393086463212967
Batch: 7. Loss: 0.16564032435417175
Batch: 8. Loss: 0.13621295988559723
Batch: 9. Loss: 0.1632315069437027
Batch: 10. Loss: 0.1334802210330963
Batch: 11. Loss: 0.15755844116210938
Batch: 12. Loss: 0.12743060290813446
Batch: 13. Loss: 0.12749430537223816
Batch: 14. Loss: 0.14636807143688202
Batch: 15. Loss: 0.16890476644039154
-----------------------------------------
Epoch completed in 910.494829416275 s.
Mean loss: 0.14700579643249512

Starting Epoch: 48, at: 1536377577.612824
=========================================
Batch: 1. Loss: 0.15539944171905518
Batch: 2. Loss: 0.1252375990152359
Batch: 3. Loss: 0.16839928925037384
Batch: 4. Loss: 0.1566322296857834
Batch: 5. Loss: 0.14570096135139465
Batch: 6. Loss: 0.15439631044864655
Batch: 7. Loss: 0.1384647786617279
Batch: 8. Loss: 0.14223477244377136
Batch: 9. Loss: 0.15338847041130066
Batch: 10. Loss: 0.14860957860946655
Batch: 11. Loss: 0.15599161386489868
Batch: 12. Loss: 0.1317777782678604
Batch: 13. Loss: 0.15681149065494537
Batch: 14. Loss: 0.1707349419593811
Batch: 15. Loss: 0.11187039315700531
-----------------------------------------
Epoch completed in 906.8628144264221 s.
Mean loss: 0.1477099508047104

Starting Epoch: 49, at: 1536378484.4756749
=========================================
Batch: 1. Loss: 0.1416858732700348
Batch: 2. Loss: 0.1536775678396225
Batch: 3. Loss: 0.14206603169441223
Batch: 4. Loss: 0.13129901885986328
Batch: 5. Loss: 0.13909584283828735
Batch: 6. Loss: 0.13169044256210327
Batch: 7. Loss: 0.14190316200256348
Batch: 8. Loss: 0.15334638953208923
Batch: 9. Loss: 0.1360078901052475
Batch: 10. Loss: 0.13770437240600586
Batch: 11. Loss: 0.1731920689344406
Batch: 12. Loss: 0.16211248934268951
Batch: 13. Loss: 0.12993130087852478
Batch: 14. Loss: 0.1346956491470337
Batch: 15. Loss: 0.13241073489189148
-----------------------------------------
Epoch completed in 914.779182434082 s.
Mean loss: 0.1427212506532669

Starting Epoch: 50, at: 1536379399.2549007
=========================================
Batch: 1. Loss: 0.1590612828731537
Batch: 2. Loss: 0.12915252149105072
Batch: 3. Loss: 0.12824732065200806
Batch: 4. Loss: 0.13814687728881836
Batch: 5. Loss: 0.11823844164609909
Batch: 6. Loss: 0.13055725395679474
Batch: 7. Loss: 0.15989156067371368
Batch: 8. Loss: 0.14519688487052917
Batch: 9. Loss: 0.12797820568084717
Batch: 10. Loss: 0.14744849503040314
Batch: 11. Loss: 0.15279574692249298
Batch: 12. Loss: 0.1525583416223526
Batch: 13. Loss: 0.14088016748428345
Batch: 14. Loss: 0.1415988802909851
Batch: 15. Loss: 0.17481467127799988
-----------------------------------------
Epoch completed in 903.9954466819763 s.
Mean loss: 0.1431044638156891

Starting Epoch: 51, at: 1536380303.2503846
=========================================
Batch: 1. Loss: 0.14079760015010834
Batch: 2. Loss: 0.1382368505001068
Batch: 3. Loss: 0.1448194980621338
Batch: 4. Loss: 0.15492060780525208
Batch: 5. Loss: 0.1256229132413864
Batch: 6. Loss: 0.14309227466583252
Batch: 7. Loss: 0.12248561531305313
Batch: 8. Loss: 0.12587149441242218
Batch: 9. Loss: 0.13710349798202515
Batch: 10. Loss: 0.152836412191391
Batch: 11. Loss: 0.16653341054916382
Batch: 12. Loss: 0.11678661406040192
Batch: 13. Loss: 0.1454407423734665
Batch: 14. Loss: 0.13848084211349487
Batch: 15. Loss: 0.1693875938653946
-----------------------------------------
Epoch completed in 905.7523939609528 s.
Mean loss: 0.14149440824985504

Starting Epoch: 52, at: 1536381209.0028105
=========================================
Batch: 1. Loss: 0.1269954890012741
Batch: 2. Loss: 0.12390253692865372
Batch: 3. Loss: 0.16481655836105347
Batch: 4. Loss: 0.12545140087604523
Batch: 5. Loss: 0.12787190079689026
Batch: 6. Loss: 0.13857148587703705
Batch: 7. Loss: 0.13699522614479065
Batch: 8. Loss: 0.12990954518318176
Batch: 9. Loss: 0.1362723857164383
Batch: 10. Loss: 0.12303322553634644
Batch: 11. Loss: 0.13559219241142273
Batch: 12. Loss: 0.14376501739025116
Batch: 13. Loss: 0.1455787867307663
Batch: 14. Loss: 0.14548729360103607
Batch: 15. Loss: 0.145114928483963
-----------------------------------------
Epoch completed in 903.5209543704987 s.
Mean loss: 0.13662387430667877

Starting Epoch: 53, at: 1536382112.5237944
=========================================
Batch: 1. Loss: 0.1159905195236206
Batch: 2. Loss: 0.16694168746471405
Batch: 3. Loss: 0.12313637882471085
Batch: 4. Loss: 0.13081899285316467
Batch: 5. Loss: 0.13765043020248413
Batch: 6. Loss: 0.12522584199905396
Batch: 7. Loss: 0.1345604658126831
Batch: 8. Loss: 0.15609554946422577
Batch: 9. Loss: 0.12908926606178284
Batch: 10. Loss: 0.15091803669929504
Batch: 11. Loss: 0.16302520036697388
Batch: 12. Loss: 0.12292123585939407
Batch: 13. Loss: 0.142811119556427
Batch: 14. Loss: 0.15870144963264465
Batch: 15. Loss: 0.14788924157619476
-----------------------------------------
Epoch completed in 910.5901062488556 s.
Mean loss: 0.1403850018978119

Starting Epoch: 54, at: 1536383023.1139271
=========================================
Batch: 1. Loss: 0.14844639599323273
Batch: 2. Loss: 0.13722975552082062
Batch: 3. Loss: 0.198782816529274
Batch: 4. Loss: 0.13524973392486572
Batch: 5. Loss: 0.16914896667003632
Batch: 6. Loss: 0.17198990285396576
Batch: 7. Loss: 0.1527646780014038
Batch: 8. Loss: 0.12510482966899872
Batch: 9. Loss: 0.1404905766248703
Batch: 10. Loss: 0.15359356999397278
Batch: 11. Loss: 0.13367006182670593
Batch: 12. Loss: 0.15127672255039215
Batch: 13. Loss: 0.12693235278129578
Batch: 14. Loss: 0.12969495356082916
Batch: 15. Loss: 0.13174842298030853
-----------------------------------------
Epoch completed in 907.0410349369049 s.
Mean loss: 0.14707492291927338

Starting Epoch: 55, at: 1536383930.154988
=========================================
Batch: 1. Loss: 0.14886362850666046
Batch: 2. Loss: 0.11929614841938019
Batch: 3. Loss: 0.12922309339046478
Batch: 4. Loss: 0.1118503287434578
Batch: 5. Loss: 0.13634757697582245
Batch: 6. Loss: 0.1314517855644226
Batch: 7. Loss: 0.12783172726631165
Batch: 8. Loss: 0.14838635921478271
Batch: 9. Loss: 0.10842001438140869
Batch: 10. Loss: 0.12716427445411682
Batch: 11. Loss: 0.1384366899728775
Batch: 12. Loss: 0.14418555796146393
Batch: 13. Loss: 0.12845385074615479
Batch: 14. Loss: 0.14228776097297668
Batch: 15. Loss: 0.14319902658462524
-----------------------------------------
Epoch completed in 903.6248345375061 s.
Mean loss: 0.1323598474264145

Starting Epoch: 56, at: 1536384833.7798657
=========================================
Batch: 1. Loss: 0.12544968724250793
Batch: 2. Loss: 0.11766964942216873
Batch: 3. Loss: 0.12942859530448914
Batch: 4. Loss: 0.15266025066375732
Batch: 5. Loss: 0.1372668594121933
Batch: 6. Loss: 0.1319098174571991
Batch: 7. Loss: 0.13246414065361023
Batch: 8. Loss: 0.12246453762054443
Batch: 9. Loss: 0.1367383897304535
Batch: 10. Loss: 0.1409124732017517
Batch: 11. Loss: 0.12291639298200607
Batch: 12. Loss: 0.12615883350372314
Batch: 13. Loss: 0.13136965036392212
Batch: 14. Loss: 0.11997155100107193
Batch: 15. Loss: 0.13329684734344482
-----------------------------------------
Epoch completed in 906.1712970733643 s.
Mean loss: 0.13071183860301971

Starting Epoch: 57, at: 1536385739.9512124
=========================================
Batch: 1. Loss: 0.10717463493347168
Batch: 2. Loss: 0.13338126242160797
Batch: 3. Loss: 0.12036829441785812
Batch: 4. Loss: 0.13310034573078156
Batch: 5. Loss: 0.11027581244707108
Batch: 6. Loss: 0.13609841465950012
Batch: 7. Loss: 0.13441957533359528
Batch: 8. Loss: 0.12508150935173035
Batch: 9. Loss: 0.13411715626716614
Batch: 10. Loss: 0.1265641152858734
Batch: 11. Loss: 0.1306794285774231
Batch: 12. Loss: 0.12102606147527695
Batch: 13. Loss: 0.10001078248023987
Batch: 14. Loss: 0.1199096143245697
Batch: 15. Loss: 0.10469915717840195
-----------------------------------------
Epoch completed in 907.0049302577972 s.
Mean loss: 0.12246040999889374

Starting Epoch: 58, at: 1536386646.9561794
=========================================
Batch: 1. Loss: 0.13159482181072235
Batch: 2. Loss: 0.14989817142486572
Batch: 3. Loss: 0.11607822775840759
Batch: 4. Loss: 0.12319503724575043
Batch: 5. Loss: 0.12493911385536194
Batch: 6. Loss: 0.12053826451301575
Batch: 7. Loss: 0.13614775240421295
Batch: 8. Loss: 0.13896167278289795
Batch: 9. Loss: 0.12041083723306656
Batch: 10. Loss: 0.12173391878604889
Batch: 11. Loss: 0.12911322712898254
Batch: 12. Loss: 0.11813875287771225
Batch: 13. Loss: 0.1300649344921112
Batch: 14. Loss: 0.12127494812011719
Batch: 15. Loss: 0.18260204792022705
-----------------------------------------
Epoch completed in 901.2410261631012 s.
Mean loss: 0.13097944855690002

Starting Epoch: 59, at: 1536387548.1972275
=========================================
Batch: 1. Loss: 0.1201087236404419
Batch: 2. Loss: 0.1177346482872963
Batch: 3. Loss: 0.13955330848693848
Batch: 4. Loss: 0.11260762810707092
Batch: 5. Loss: 0.1261606067419052
Batch: 6. Loss: 0.10146796703338623
Batch: 7. Loss: 0.12132330238819122
Batch: 8. Loss: 0.12407604604959488
Batch: 9. Loss: 0.10083842277526855
Batch: 10. Loss: 0.13620343804359436
Batch: 11. Loss: 0.13615348935127258
Batch: 12. Loss: 0.10743867605924606
Batch: 13. Loss: 0.1157008707523346
Batch: 14. Loss: 0.11610733717679977
Batch: 15. Loss: 0.12752117216587067
-----------------------------------------
Epoch completed in 919.5299401283264 s.
Mean loss: 0.12019970268011093

Starting Epoch: 60, at: 1536388467.7271929
=========================================
Batch: 1. Loss: 0.1067214086651802
Batch: 2. Loss: 0.11728224158287048
Batch: 3. Loss: 0.10648806393146515
Batch: 4. Loss: 0.10998573899269104
Batch: 5. Loss: 0.09865639358758926
Batch: 6. Loss: 0.10605990886688232
Batch: 7. Loss: 0.13113993406295776
Batch: 8. Loss: 0.11704491823911667
Batch: 9. Loss: 0.1351134181022644
Batch: 10. Loss: 0.11523912101984024
Batch: 11. Loss: 0.11405742913484573
Batch: 12. Loss: 0.11424442380666733
Batch: 13. Loss: 0.11130788177251816
Batch: 14. Loss: 0.1172131597995758
Batch: 15. Loss: 0.08330278098583221
-----------------------------------------
Epoch completed in 908.3332126140594 s.
Mean loss: 0.11225711554288864

Starting Epoch: 61, at: 1536389376.060433
=========================================
Batch: 1. Loss: 0.12351387739181519
Batch: 2. Loss: 0.09424273669719696
Batch: 3. Loss: 0.11486829817295074
Batch: 4. Loss: 0.10657365620136261
Batch: 5. Loss: 0.11602579802274704
Batch: 6. Loss: 0.10015291720628738
Batch: 7. Loss: 0.12818072736263275
Batch: 8. Loss: 0.1025848537683487
Batch: 9. Loss: 0.10521827638149261
Batch: 10. Loss: 0.1126687228679657
Batch: 11. Loss: 0.1156201884150505
Batch: 12. Loss: 0.11870754510164261
Batch: 13. Loss: 0.1075541153550148
Batch: 14. Loss: 0.11304133385419846
Batch: 15. Loss: 0.109610915184021
-----------------------------------------
Epoch completed in 910.0996177196503 s.
Mean loss: 0.11123759299516678

Starting Epoch: 62, at: 1536390286.1600804
=========================================
Batch: 1. Loss: 0.11900210380554199
Batch: 2. Loss: 0.12427990138530731
Batch: 3. Loss: 0.11090395599603653
Batch: 4. Loss: 0.1244734525680542
Batch: 5. Loss: 0.10129492729902267
Batch: 6. Loss: 0.09019989520311356
Batch: 7. Loss: 0.12071660161018372
Batch: 8. Loss: 0.11148662865161896
Batch: 9. Loss: 0.10158329457044601
Batch: 10. Loss: 0.09966954588890076
Batch: 11. Loss: 0.10135447978973389
Batch: 12. Loss: 0.12206321954727173
Batch: 13. Loss: 0.08747406303882599
Batch: 14. Loss: 0.08309947699308395
Batch: 15. Loss: 0.12858328223228455
-----------------------------------------
Epoch completed in 905.5280487537384 s.
Mean loss: 0.10841233283281326

Starting Epoch: 63, at: 1536391191.688156
=========================================
Batch: 1. Loss: 0.11680633574724197
Batch: 2. Loss: 0.08747176826000214
Batch: 3. Loss: 0.12768656015396118
Batch: 4. Loss: 0.10271517932415009
Batch: 5. Loss: 0.10376157611608505
Batch: 6. Loss: 0.11117792874574661
Batch: 7. Loss: 0.12356801331043243
Batch: 8. Loss: 0.09811563789844513
Batch: 9. Loss: 0.13519610464572906
Batch: 10. Loss: 0.08733686804771423
Batch: 11. Loss: 0.09132835268974304
Batch: 12. Loss: 0.10397550463676453
Batch: 13. Loss: 0.12121632695198059
Batch: 14. Loss: 0.10177511721849442
Batch: 15. Loss: 0.10836504399776459
-----------------------------------------
Epoch completed in 908.7829256057739 s.
Mean loss: 0.10803309828042984

Starting Epoch: 64, at: 1536392100.4712372
=========================================
Batch: 1. Loss: 0.11010561138391495
Batch: 2. Loss: 0.08533964306116104
Batch: 3. Loss: 0.10199694335460663
Batch: 4. Loss: 0.09648717194795609
Batch: 5. Loss: 0.11926037818193436
Batch: 6. Loss: 0.09669411182403564
Batch: 7. Loss: 0.11131678521633148
Batch: 8. Loss: 0.10171875357627869
Batch: 9. Loss: 0.0935344249010086
Batch: 10. Loss: 0.10031533241271973
Batch: 11. Loss: 0.09268183261156082
Batch: 12. Loss: 0.09375327825546265
Batch: 13. Loss: 0.09865298867225647
Batch: 14. Loss: 0.09080066531896591
Batch: 15. Loss: 0.09931273013353348
-----------------------------------------
Epoch completed in 937.2197706699371 s.
Mean loss: 0.09946472197771072

Starting Epoch: 65, at: 1536393037.6910346
=========================================
Batch: 1. Loss: 0.10714823752641678
Batch: 2. Loss: 0.09509216248989105
Batch: 3. Loss: 0.10093605518341064
Batch: 4. Loss: 0.08631069958209991
Batch: 5. Loss: 0.08683444559574127
Batch: 6. Loss: 0.11233112215995789
Batch: 7. Loss: 0.07976415008306503
Batch: 8. Loss: 0.07782150059938431
Batch: 9. Loss: 0.0967315286397934
Batch: 10. Loss: 0.1008910983800888
Batch: 11. Loss: 0.08192987740039825
Batch: 12. Loss: 0.09801007062196732
Batch: 13. Loss: 0.09502433240413666
Batch: 14. Loss: 0.0913851410150528
Batch: 15. Loss: 0.12513355910778046
-----------------------------------------
Epoch completed in 1035.696580171585 s.
Mean loss: 0.09568960219621658

Starting Epoch: 66, at: 1536394073.3878345
=========================================
Batch: 1. Loss: 0.08750218152999878
Batch: 2. Loss: 0.09036524593830109
Batch: 3. Loss: 0.11697211116552353
Batch: 4. Loss: 0.102790966629982
Batch: 5. Loss: 0.09844591468572617
Batch: 6. Loss: 0.09466566145420074
Batch: 7. Loss: 0.09413564205169678
Batch: 8. Loss: 0.0896134302020073
Batch: 9. Loss: 0.09186152368783951
Batch: 10. Loss: 0.0894780158996582
Batch: 11. Loss: 0.09249471127986908
Batch: 12. Loss: 0.097802072763443
Batch: 13. Loss: 0.09240011125802994
Batch: 14. Loss: 0.09762389957904816
Batch: 15. Loss: 0.10321629047393799
-----------------------------------------
Epoch completed in 960.663631439209 s.
Mean loss: 0.09595785290002823

Starting Epoch: 67, at: 1536395034.051499
=========================================
Batch: 1. Loss: 0.0879790186882019
Batch: 2. Loss: 0.10327223688364029
Batch: 3. Loss: 0.09065088629722595
Batch: 4. Loss: 0.07331671565771103
Batch: 5. Loss: 0.08516079187393188
Batch: 6. Loss: 0.09843745082616806
Batch: 7. Loss: 0.0887439101934433
Batch: 8. Loss: 0.10390221327543259
Batch: 9. Loss: 0.09238719195127487
Batch: 10. Loss: 0.08224226534366608
Batch: 11. Loss: 0.07043622434139252
Batch: 12. Loss: 0.08125767856836319
Batch: 13. Loss: 0.0991492047905922
Batch: 14. Loss: 0.08663976937532425
Batch: 15. Loss: 0.0653100460767746
-----------------------------------------
Epoch completed in 909.2468702793121 s.
Mean loss: 0.08725903928279877

Starting Epoch: 68, at: 1536395943.2984066
=========================================
Batch: 1. Loss: 0.08028609305620193
Batch: 2. Loss: 0.07652992755174637
Batch: 3. Loss: 0.09372775256633759
Batch: 4. Loss: 0.09099322557449341
Batch: 5. Loss: 0.08585432916879654
Batch: 6. Loss: 0.06959044933319092
Batch: 7. Loss: 0.08261030912399292
Batch: 8. Loss: 0.09348134696483612
Batch: 9. Loss: 0.09598760306835175
Batch: 10. Loss: 0.09267861396074295
Batch: 11. Loss: 0.07872798293828964
Batch: 12. Loss: 0.08148185163736343
Batch: 13. Loss: 0.07498045265674591
Batch: 14. Loss: 0.07909124344587326
Batch: 15. Loss: 0.06746739894151688
-----------------------------------------
Epoch completed in 910.1754579544067 s.
Mean loss: 0.08289923518896103

Starting Epoch: 69, at: 1536396853.4738975
=========================================
Batch: 1. Loss: 0.06877356767654419
Batch: 2. Loss: 0.07704120874404907
Batch: 3. Loss: 0.08978696912527084
Batch: 4. Loss: 0.0869177058339119
Batch: 5. Loss: 0.08766007423400879
Batch: 6. Loss: 0.06399061530828476
Batch: 7. Loss: 0.07528273016214371
Batch: 8. Loss: 0.08727020770311356
Batch: 9. Loss: 0.08398260921239853
Batch: 10. Loss: 0.08291685581207275
Batch: 11. Loss: 0.07645396143198013
Batch: 12. Loss: 0.07397288084030151
Batch: 13. Loss: 0.07859326153993607
Batch: 14. Loss: 0.07133801281452179
Batch: 15. Loss: 0.07329759001731873
-----------------------------------------
Epoch completed in 904.2058181762695 s.
Mean loss: 0.07848522067070007

Starting Epoch: 70, at: 1536397757.67975
=========================================
Batch: 1. Loss: 0.06794223189353943
Batch: 2. Loss: 0.10649941116571426
Batch: 3. Loss: 0.07299546897411346
Batch: 4. Loss: 0.07185765355825424
Batch: 5. Loss: 0.0680820569396019
Batch: 6. Loss: 0.07180121541023254
Batch: 7. Loss: 0.07366670668125153
Batch: 8. Loss: 0.07448450475931168
Batch: 9. Loss: 0.08450023084878922
Batch: 10. Loss: 0.07192328572273254
Batch: 11. Loss: 0.0673154965043068
Batch: 12. Loss: 0.07359451055526733
Batch: 13. Loss: 0.07302893698215485
Batch: 14. Loss: 0.07327589392662048
Batch: 15. Loss: 0.0750708356499672
-----------------------------------------
Epoch completed in 903.0742616653442 s.
Mean loss: 0.07506923377513885

Starting Epoch: 71, at: 1536398660.7540457
=========================================
Batch: 1. Loss: 0.07693153619766235
Batch: 2. Loss: 0.06724268943071365
Batch: 3. Loss: 0.0801842212677002
Batch: 4. Loss: 0.0685712993144989
Batch: 5. Loss: 0.06801749765872955
Batch: 6. Loss: 0.07063529640436172
Batch: 7. Loss: 0.07188699394464493
Batch: 8. Loss: 0.07863103598356247
Batch: 9. Loss: 0.0620904378592968
Batch: 10. Loss: 0.07820110023021698
Batch: 11. Loss: 0.07217282056808472
Batch: 12. Loss: 0.07458694279193878
Batch: 13. Loss: 0.07150331884622574
Batch: 14. Loss: 0.07690590620040894
Batch: 15. Loss: 0.06571082770824432
-----------------------------------------
Epoch completed in 906.9870386123657 s.
Mean loss: 0.07221813499927521

Starting Epoch: 72, at: 1536399567.7411795
=========================================
Batch: 1. Loss: 0.0703740268945694
Batch: 2. Loss: 0.07507199048995972
Batch: 3. Loss: 0.07851157337427139
Batch: 4. Loss: 0.07097973674535751
Batch: 5. Loss: 0.06779995560646057
Batch: 6. Loss: 0.07627467066049576
Batch: 7. Loss: 0.06202283129096031
Batch: 8. Loss: 0.06974587589502335
Batch: 9. Loss: 0.07191604375839233
Batch: 10. Loss: 0.06577732414007187
Batch: 11. Loss: 0.06528003513813019
Batch: 12. Loss: 0.06828516721725464
Batch: 13. Loss: 0.06795427948236465
Batch: 14. Loss: 0.06538468599319458
Batch: 15. Loss: 0.06654918938875198
-----------------------------------------
Epoch completed in 905.8401954174042 s.
Mean loss: 0.06946182250976562

Starting Epoch: 73, at: 1536400473.5814118
=========================================
Batch: 1. Loss: 0.06486556679010391
Batch: 2. Loss: 0.06760496646165848
Batch: 3. Loss: 0.0736980065703392
Batch: 4. Loss: 0.059665482491254807
Batch: 5. Loss: 0.07771115005016327
Batch: 6. Loss: 0.06475786864757538
Batch: 7. Loss: 0.06696872413158417
Batch: 8. Loss: 0.07297658175230026
Batch: 9. Loss: 0.06407389044761658
Batch: 10. Loss: 0.07190689444541931
Batch: 11. Loss: 0.06375762820243835
Batch: 12. Loss: 0.07385615259408951
Batch: 13. Loss: 0.06915011256933212
Batch: 14. Loss: 0.061567552387714386
Batch: 15. Loss: 0.07816604524850845
-----------------------------------------
Epoch completed in 904.54181599617 s.
Mean loss: 0.06871511042118073

Starting Epoch: 74, at: 1536401378.1232536
=========================================
Batch: 1. Loss: 0.07438351213932037
Batch: 2. Loss: 0.055598195642232895
Batch: 3. Loss: 0.067798912525177
Batch: 4. Loss: 0.06022766977548599
Batch: 5. Loss: 0.07370249181985855
Batch: 6. Loss: 0.07218379527330399
Batch: 7. Loss: 0.07207705080509186
Batch: 8. Loss: 0.07562392204999924
Batch: 9. Loss: 0.0672038197517395
Batch: 10. Loss: 0.05960700660943985
Batch: 11. Loss: 0.06217952445149422
Batch: 12. Loss: 0.05804397538304329
Batch: 13. Loss: 0.06685949862003326
Batch: 14. Loss: 0.06036341190338135
Batch: 15. Loss: 0.08024448156356812
-----------------------------------------
Epoch completed in 908.5233602523804 s.
Mean loss: 0.06707315146923065

Starting Epoch: 75, at: 1536402286.6466436
=========================================
Batch: 1. Loss: 0.057343874126672745
Batch: 2. Loss: 0.05995458737015724
Batch: 3. Loss: 0.0664948970079422
Batch: 4. Loss: 0.06530365347862244
Batch: 5. Loss: 0.06519501656293869
Batch: 6. Loss: 0.06464577466249466
Batch: 7. Loss: 0.08595253527164459
Batch: 8. Loss: 0.06401438266038895
Batch: 9. Loss: 0.06518039852380753
Batch: 10. Loss: 0.07717213779687881
Batch: 11. Loss: 0.07470154017210007
Batch: 12. Loss: 0.06515845656394958
Batch: 13. Loss: 0.059722959995269775
Batch: 14. Loss: 0.06889607012271881
Batch: 15. Loss: 0.05996933579444885
-----------------------------------------
Epoch completed in 903.9118061065674 s.
Mean loss: 0.06664704531431198

Starting Epoch: 76, at: 1536403190.5584793
=========================================
Batch: 1. Loss: 0.06752341240644455
Batch: 2. Loss: 0.05915733054280281
Batch: 3. Loss: 0.06376685947179794
Batch: 4. Loss: 0.05750134214758873
Batch: 5. Loss: 0.06132715940475464
Batch: 6. Loss: 0.0623561255633831
Batch: 7. Loss: 0.06689456850290298
Batch: 8. Loss: 0.06641091406345367
Batch: 9. Loss: 0.07297336310148239
Batch: 10. Loss: 0.07068242132663727
Batch: 11. Loss: 0.07399706542491913
Batch: 12. Loss: 0.061961401253938675
Batch: 13. Loss: 0.06251814216375351
Batch: 14. Loss: 0.07020052522420883
Batch: 15. Loss: 0.0600285530090332
-----------------------------------------
Epoch completed in 907.5709154605865 s.
Mean loss: 0.06515327841043472

Starting Epoch: 77, at: 1536404098.1296115
=========================================
Batch: 1. Loss: 0.07516079396009445
Batch: 2. Loss: 0.060923267155885696
Batch: 3. Loss: 0.055901043117046356
Batch: 4. Loss: 0.06702065467834473
Batch: 5. Loss: 0.06738930940628052
Batch: 6. Loss: 0.058865536004304886
Batch: 7. Loss: 0.07419426739215851
Batch: 8. Loss: 0.06511874496936798
Batch: 9. Loss: 0.04790743440389633
Batch: 10. Loss: 0.07043556123971939
Batch: 11. Loss: 0.07257962226867676
Batch: 12. Loss: 0.061722368001937866
Batch: 13. Loss: 0.06888339668512344
Batch: 14. Loss: 0.05312594026327133
Batch: 15. Loss: 0.07271343469619751
-----------------------------------------
Epoch completed in 906.2917332649231 s.
Mean loss: 0.0647960901260376

Starting Epoch: 78, at: 1536405004.4213815
=========================================
Batch: 1. Loss: 0.0598580539226532
Batch: 2. Loss: 0.07361924648284912
Batch: 3. Loss: 0.05623844638466835
Batch: 4. Loss: 0.06042497605085373
Batch: 5. Loss: 0.05559249594807625
Batch: 6. Loss: 0.05945855751633644
Batch: 7. Loss: 0.07119928300380707
Batch: 8. Loss: 0.05794962868094444
Batch: 9. Loss: 0.061100468039512634
Batch: 10. Loss: 0.0511702224612236
Batch: 11. Loss: 0.05526409670710564
Batch: 12. Loss: 0.06735281646251678
Batch: 13. Loss: 0.07037832587957382
Batch: 14. Loss: 0.05454929918050766
Batch: 15. Loss: 0.06139107048511505
-----------------------------------------
Epoch completed in 901.251612663269 s.
Mean loss: 0.06103646382689476

Starting Epoch: 79, at: 1536405905.6730328
=========================================
Batch: 1. Loss: 0.05620962381362915
Batch: 2. Loss: 0.06166953966021538
Batch: 3. Loss: 0.056769147515296936
Batch: 4. Loss: 0.05432635918259621
Batch: 5. Loss: 0.058025605976581573
Batch: 6. Loss: 0.050848379731178284
Batch: 7. Loss: 0.07231011241674423
Batch: 8. Loss: 0.05368584394454956
Batch: 9. Loss: 0.0632818341255188
Batch: 10. Loss: 0.053284794092178345
Batch: 11. Loss: 0.0634586438536644
Batch: 12. Loss: 0.05471773073077202
Batch: 13. Loss: 0.061690133064985275
Batch: 14. Loss: 0.05429140850901604
Batch: 15. Loss: 0.06603344529867172
-----------------------------------------
Epoch completed in 900.2295932769775 s.
Mean loss: 0.05870683491230011

Starting Epoch: 80, at: 1536406805.9026637
=========================================
Batch: 1. Loss: 0.061745885759592056
Batch: 2. Loss: 0.044750332832336426
Batch: 3. Loss: 0.05341467261314392
Batch: 4. Loss: 0.05044019967317581
Batch: 5. Loss: 0.05910253897309303
Batch: 6. Loss: 0.05632292106747627
Batch: 7. Loss: 0.0676833912730217
Batch: 8. Loss: 0.05134011059999466
Batch: 9. Loss: 0.058746226131916046
Batch: 10. Loss: 0.059323377907276154
Batch: 11. Loss: 0.05486061051487923
Batch: 12. Loss: 0.06692317873239517
Batch: 13. Loss: 0.05645715445280075
Batch: 14. Loss: 0.06691969186067581
Batch: 15. Loss: 0.052106522023677826
-----------------------------------------
Epoch completed in 910.6691071987152 s.
Mean loss: 0.05734245479106903

Starting Epoch: 81, at: 1536407716.5718064
=========================================
Batch: 1. Loss: 0.06405803561210632
Batch: 2. Loss: 0.0586390420794487
Batch: 3. Loss: 0.05338337644934654
Batch: 4. Loss: 0.05629011616110802
Batch: 5. Loss: 0.05264351889491081
Batch: 6. Loss: 0.06029229238629341
Batch: 7. Loss: 0.06439745426177979
Batch: 8. Loss: 0.05265073850750923
Batch: 9. Loss: 0.05582217499613762
Batch: 10. Loss: 0.05847889184951782
Batch: 11. Loss: 0.047494638711214066
Batch: 12. Loss: 0.05389837920665741
Batch: 13. Loss: 0.051680441945791245
Batch: 14. Loss: 0.05841449648141861
Batch: 15. Loss: 0.06168939173221588
-----------------------------------------
Epoch completed in 899.9552662372589 s.
Mean loss: 0.056655533611774445

Starting Epoch: 82, at: 1536408616.5271027
=========================================
Batch: 1. Loss: 0.05375785008072853
Batch: 2. Loss: 0.047915734350681305
Batch: 3. Loss: 0.06333962827920914
Batch: 4. Loss: 0.057552073150873184
Batch: 5. Loss: 0.053491510450839996
Batch: 6. Loss: 0.06041533872485161
Batch: 7. Loss: 0.05271676927804947
Batch: 8. Loss: 0.052516285330057144
Batch: 9. Loss: 0.05377458781003952
Batch: 10. Loss: 0.055678870528936386
Batch: 11. Loss: 0.04736247658729553
Batch: 12. Loss: 0.05390656366944313
Batch: 13. Loss: 0.058348167687654495
Batch: 14. Loss: 0.052808728069067
Batch: 15. Loss: 0.058043502271175385
-----------------------------------------
Epoch completed in 906.0409474372864 s.
Mean loss: 0.05477520078420639

Starting Epoch: 83, at: 1536409522.5680864
=========================================
Batch: 1. Loss: 0.05209987983107567
Batch: 2. Loss: 0.06431671231985092
Batch: 3. Loss: 0.04925873503088951
Batch: 4. Loss: 0.0565948523581028
Batch: 5. Loss: 0.05207747220993042
Batch: 6. Loss: 0.057604387402534485
Batch: 7. Loss: 0.0546066015958786
Batch: 8. Loss: 0.05086968466639519
Batch: 9. Loss: 0.06041298806667328
Batch: 10. Loss: 0.051066625863313675
Batch: 11. Loss: 0.05582340806722641
Batch: 12. Loss: 0.0536210723221302
Batch: 13. Loss: 0.0474955253303051
Batch: 14. Loss: 0.04472988471388817
Batch: 15. Loss: 0.050753962248563766
-----------------------------------------
Epoch completed in 905.3282489776611 s.
Mean loss: 0.05342211574316025

Starting Epoch: 84, at: 1536410427.8963616
=========================================
Batch: 1. Loss: 0.05093192309141159
Batch: 2. Loss: 0.0478060245513916
Batch: 3. Loss: 0.05779694765806198
Batch: 4. Loss: 0.052257291972637177
Batch: 5. Loss: 0.05422791466116905
Batch: 6. Loss: 0.050089772790670395
Batch: 7. Loss: 0.05007302016019821
Batch: 8. Loss: 0.056840479373931885
Batch: 9. Loss: 0.057014551013708115
Batch: 10. Loss: 0.045825812965631485
Batch: 11. Loss: 0.0528620146214962
Batch: 12. Loss: 0.05297788977622986
Batch: 13. Loss: 0.045622941106557846
Batch: 14. Loss: 0.052779652178287506
Batch: 15. Loss: 0.04369423910975456
-----------------------------------------
Epoch completed in 908.3149931430817 s.
Mean loss: 0.051386699080467224

Starting Epoch: 85, at: 1536411336.2113895
=========================================
Batch: 1. Loss: 0.043720636516809464
Batch: 2. Loss: 0.05613730475306511
Batch: 3. Loss: 0.04945117235183716
Batch: 4. Loss: 0.04894300550222397
Batch: 5. Loss: 0.04539595544338226
Batch: 6. Loss: 0.05157516151666641
Batch: 7. Loss: 0.04917595908045769
Batch: 8. Loss: 0.05072474107146263
Batch: 9. Loss: 0.048960987478494644
Batch: 10. Loss: 0.05143049731850624
Batch: 11. Loss: 0.05027587339282036
Batch: 12. Loss: 0.05453749746084213
Batch: 13. Loss: 0.04786846414208412
Batch: 14. Loss: 0.0537048764526844
Batch: 15. Loss: 0.05957403406500816
-----------------------------------------
Epoch completed in 903.72873878479 s.
Mean loss: 0.05076507478952408

Starting Epoch: 86, at: 1536412239.9402184
=========================================
Batch: 1. Loss: 0.0433419831097126
Batch: 2. Loss: 0.04072900488972664
Batch: 3. Loss: 0.046417754143476486
Batch: 4. Loss: 0.05937778204679489
Batch: 5. Loss: 0.052534542977809906
Batch: 6. Loss: 0.058199673891067505
Batch: 7. Loss: 0.0480651929974556
Batch: 8. Loss: 0.049608975648880005
Batch: 9. Loss: 0.05081699416041374
Batch: 10. Loss: 0.048198401927948
Batch: 11. Loss: 0.052895791828632355
Batch: 12. Loss: 0.048388488590717316
Batch: 13. Loss: 0.0481119379401207
Batch: 14. Loss: 0.04735201597213745
Batch: 15. Loss: 0.045936547219753265
-----------------------------------------
Epoch completed in 904.452894449234 s.
Mean loss: 0.0493316687643528

Starting Epoch: 87, at: 1536413144.3932638
=========================================
Batch: 1. Loss: 0.04325050860643387
Batch: 2. Loss: 0.04271826893091202
Batch: 3. Loss: 0.0536256805062294
Batch: 4. Loss: 0.046551745384931564
Batch: 5. Loss: 0.04739830270409584
Batch: 6. Loss: 0.055757638067007065
Batch: 7. Loss: 0.0452725812792778
Batch: 8. Loss: 0.04879499599337578
Batch: 9. Loss: 0.041730862110853195
Batch: 10. Loss: 0.05259129777550697
Batch: 11. Loss: 0.05155706778168678
Batch: 12. Loss: 0.05949592590332031
Batch: 13. Loss: 0.04786021634936333
Batch: 14. Loss: 0.041507259011268616
Batch: 15. Loss: 0.04857959225773811
-----------------------------------------
Epoch completed in 910.3726723194122 s.
Mean loss: 0.04844612628221512

Starting Epoch: 88, at: 1536414054.7660613
=========================================
Batch: 1. Loss: 0.0480465367436409
Batch: 2. Loss: 0.04747847095131874
Batch: 3. Loss: 0.051326632499694824
Batch: 4. Loss: 0.04156186431646347
Batch: 5. Loss: 0.044283896684646606
Batch: 6. Loss: 0.046625763177871704
Batch: 7. Loss: 0.04288763925433159
Batch: 8. Loss: 0.050393715500831604
Batch: 9. Loss: 0.05921493098139763
Batch: 10. Loss: 0.047141097486019135
Batch: 11. Loss: 0.049453429877758026
Batch: 12. Loss: 0.050777092576026917
Batch: 13. Loss: 0.03626479580998421
Batch: 14. Loss: 0.04859153926372528
Batch: 15. Loss: 0.04591498151421547
-----------------------------------------
Epoch completed in 909.1602921485901 s.
Mean loss: 0.0473308227956295

Starting Epoch: 89, at: 1536414963.926382
=========================================
Batch: 1. Loss: 0.04663907736539841
Batch: 2. Loss: 0.04257018119096756
Batch: 3. Loss: 0.043744202703237534
Batch: 4. Loss: 0.05136725679039955
Batch: 5. Loss: 0.059695180505514145
Batch: 6. Loss: 0.03921375423669815
Batch: 7. Loss: 0.04978739470243454
Batch: 8. Loss: 0.04752963036298752
Batch: 9. Loss: 0.0445912703871727
Batch: 10. Loss: 0.03414696827530861
Batch: 11. Loss: 0.04989293962717056
Batch: 12. Loss: 0.04559185355901718
Batch: 13. Loss: 0.04407062008976936
Batch: 14. Loss: 0.04607102647423744
Batch: 15. Loss: 0.06389418244361877
-----------------------------------------
Epoch completed in 911.440806388855 s.
Mean loss: 0.04725370556116104

Starting Epoch: 90, at: 1536415875.3673208
=========================================
Batch: 1. Loss: 0.04069722443819046
Batch: 2. Loss: 0.045823536813259125
Batch: 3. Loss: 0.04141588509082794
Batch: 4. Loss: 0.05139941722154617
Batch: 5. Loss: 0.0472421869635582
Batch: 6. Loss: 0.04481920599937439
Batch: 7. Loss: 0.05681486055254936
Batch: 8. Loss: 0.046782564371824265
Batch: 9. Loss: 0.04664313420653343
Batch: 10. Loss: 0.049263399094343185
Batch: 11. Loss: 0.04449823126196861
Batch: 12. Loss: 0.05079646781086922
Batch: 13. Loss: 0.04025808721780777
Batch: 14. Loss: 0.037544284015893936
Batch: 15. Loss: 0.04368811845779419
-----------------------------------------
Epoch completed in 903.2055678367615 s.
Mean loss: 0.045845773071050644

Starting Epoch: 91, at: 1536416778.5731115
=========================================
Batch: 1. Loss: 0.05155915021896362
Batch: 2. Loss: 0.04676837474107742
Batch: 3. Loss: 0.043679554015398026
Batch: 4. Loss: 0.03885688632726669
Batch: 5. Loss: 0.04736720770597458
Batch: 6. Loss: 0.057113900780677795
Batch: 7. Loss: 0.04671696946024895
Batch: 8. Loss: 0.040394604206085205
Batch: 9. Loss: 0.04542343318462372
Batch: 10. Loss: 0.04024071618914604
Batch: 11. Loss: 0.042010869830846786
Batch: 12. Loss: 0.04334816709160805
Batch: 13. Loss: 0.03977595269680023
Batch: 14. Loss: 0.044324085116386414
Batch: 15. Loss: 0.04461648315191269
-----------------------------------------
Epoch completed in 915.6245107650757 s.
Mean loss: 0.044813092797994614

Starting Epoch: 92, at: 1536417694.197648
=========================================
Batch: 1. Loss: 0.05056136101484299
Batch: 2. Loss: 0.03862159699201584
Batch: 3. Loss: 0.057564154267311096
Batch: 4. Loss: 0.04541580379009247
Batch: 5. Loss: 0.04536125063896179
Batch: 6. Loss: 0.041663818061351776
Batch: 7. Loss: 0.04056414216756821
Batch: 8. Loss: 0.04374910145998001
Batch: 9. Loss: 0.03919994831085205
Batch: 10. Loss: 0.04383905977010727
Batch: 11. Loss: 0.04354904219508171
Batch: 12. Loss: 0.04732146114110947
Batch: 13. Loss: 0.04634925723075867
Batch: 14. Loss: 0.03811783716082573
Batch: 15. Loss: 0.04009772464632988
-----------------------------------------
Epoch completed in 903.4345207214355 s.
Mean loss: 0.04413170367479324

Starting Epoch: 93, at: 1536418597.6323931
=========================================
Batch: 1. Loss: 0.0413145013153553
Batch: 2. Loss: 0.03982888162136078
Batch: 3. Loss: 0.042210862040519714
Batch: 4. Loss: 0.042937226593494415
Batch: 5. Loss: 0.046730391681194305
Batch: 6. Loss: 0.03793826699256897
Batch: 7. Loss: 0.03800438717007637
Batch: 8. Loss: 0.04692486673593521
Batch: 9. Loss: 0.047645095735788345
Batch: 10. Loss: 0.04099901020526886
Batch: 11. Loss: 0.04531393200159073
Batch: 12. Loss: 0.04395565390586853
Batch: 13. Loss: 0.05262942984700203
Batch: 14. Loss: 0.04828488826751709
Batch: 15. Loss: 0.05209289491176605
-----------------------------------------
Epoch completed in 903.2906076908112 s.
Mean loss: 0.04445401951670647

Starting Epoch: 94, at: 1536419500.9230382
=========================================
Batch: 1. Loss: 0.03858276829123497
Batch: 2. Loss: 0.04308224096894264
Batch: 3. Loss: 0.0466795340180397
Batch: 4. Loss: 0.05068834871053696
Batch: 5. Loss: 0.03726087883114815
Batch: 6. Loss: 0.043279461562633514
Batch: 7. Loss: 0.04237738624215126
Batch: 8. Loss: 0.04455622658133507
Batch: 9. Loss: 0.040113769471645355
Batch: 10. Loss: 0.04184745252132416
Batch: 11. Loss: 0.052511151880025864
Batch: 12. Loss: 0.0455196350812912
Batch: 13. Loss: 0.042134542018175125
Batch: 14. Loss: 0.040885161608457565
Batch: 15. Loss: 0.04062288627028465
-----------------------------------------
Epoch completed in 900.7529301643372 s.
Mean loss: 0.04334276169538498

Starting Epoch: 95, at: 1536420401.6759984
=========================================
Batch: 1. Loss: 0.04168252646923065
Batch: 2. Loss: 0.0490432046353817
Batch: 3. Loss: 0.036225009709596634
Batch: 4. Loss: 0.046792637556791306
Batch: 5. Loss: 0.0336458645761013
Batch: 6. Loss: 0.042795948684215546
Batch: 7. Loss: 0.04136836156249046
Batch: 8. Loss: 0.03899266570806503
Batch: 9. Loss: 0.04768989607691765
Batch: 10. Loss: 0.04041527584195137
Batch: 11. Loss: 0.03958012908697128
Batch: 12. Loss: 0.04903624579310417
Batch: 13. Loss: 0.03830910474061966
Batch: 14. Loss: 0.0438963882625103
Batch: 15. Loss: 0.049969665706157684
-----------------------------------------
Epoch completed in 897.3994204998016 s.
Mean loss: 0.04262952879071236

Starting Epoch: 96, at: 1536421299.0754497
=========================================
Batch: 1. Loss: 0.04189305752515793
Batch: 2. Loss: 0.04899640381336212
Batch: 3. Loss: 0.043727993965148926
Batch: 4. Loss: 0.04000220447778702
Batch: 5. Loss: 0.04287626966834068
Batch: 6. Loss: 0.03946077823638916
Batch: 7. Loss: 0.039222050458192825
Batch: 8. Loss: 0.03681595250964165
Batch: 9. Loss: 0.04073251411318779
Batch: 10. Loss: 0.040656864643096924
Batch: 11. Loss: 0.04379527270793915
Batch: 12. Loss: 0.04303131625056267
Batch: 13. Loss: 0.04137476161122322
Batch: 14. Loss: 0.04182647541165352
Batch: 15. Loss: 0.046960990875959396
-----------------------------------------
Epoch completed in 918.4608120918274 s.
Mean loss: 0.042091529816389084

Starting Epoch: 97, at: 1536422217.536357
=========================================
Batch: 1. Loss: 0.040203794836997986
Batch: 2. Loss: 0.04416631907224655
Batch: 3. Loss: 0.042649831622838974
Batch: 4. Loss: 0.04642968252301216
Batch: 5. Loss: 0.043967556208372116
Batch: 6. Loss: 0.04284166917204857
Batch: 7. Loss: 0.038481611758470535
Batch: 8. Loss: 0.0398169606924057
Batch: 9. Loss: 0.0460052564740181
Batch: 10. Loss: 0.045012496411800385
Batch: 11. Loss: 0.03350112587213516
Batch: 12. Loss: 0.03639286011457443
Batch: 13. Loss: 0.04072069749236107
Batch: 14. Loss: 0.046257469803094864
Batch: 15. Loss: 0.039363324642181396
-----------------------------------------
Epoch completed in 908.0892629623413 s.
Mean loss: 0.041720710694789886

Starting Epoch: 98, at: 1536423125.6256459
=========================================
Batch: 1. Loss: 0.03561992943286896
Batch: 2. Loss: 0.04505293816328049
Batch: 3. Loss: 0.03780903294682503
Batch: 4. Loss: 0.05094999447464943
Batch: 5. Loss: 0.04042598232626915
Batch: 6. Loss: 0.04149450361728668
Batch: 7. Loss: 0.04442594572901726
Batch: 8. Loss: 0.03827233240008354
Batch: 9. Loss: 0.04280516877770424
Batch: 10. Loss: 0.048254869878292084
Batch: 11. Loss: 0.03204809129238129
Batch: 12. Loss: 0.042121462523937225
Batch: 13. Loss: 0.03468839451670647
Batch: 14. Loss: 0.037969715893268585
Batch: 15. Loss: 0.04312741011381149
-----------------------------------------
Epoch completed in 903.8996617794037 s.
Mean loss: 0.04100438579916954

Starting Epoch: 99, at: 1536424029.5253375
=========================================
Batch: 1. Loss: 0.040810175240039825
Batch: 2. Loss: 0.03883485496044159
Batch: 3. Loss: 0.04498020559549332
Batch: 4. Loss: 0.04703957587480545
Batch: 5. Loss: 0.043980978429317474
Batch: 6. Loss: 0.03435627743601799
Batch: 7. Loss: 0.04279308766126633
Batch: 8. Loss: 0.04213405027985573
Batch: 9. Loss: 0.04060927778482437
Batch: 10. Loss: 0.03565078228712082
Batch: 11. Loss: 0.03391943499445915
Batch: 12. Loss: 0.04774000123143196
Batch: 13. Loss: 0.03902812674641609
Batch: 14. Loss: 0.03254309296607971
Batch: 15. Loss: 0.040011484175920486
-----------------------------------------
Epoch completed in 910.0125784873962 s.
Mean loss: 0.040295422077178955

Starting Epoch: 100, at: 1536424939.5381224
=========================================
Batch: 1. Loss: 0.03881790116429329
Batch: 2. Loss: 0.03978674113750458
Batch: 3. Loss: 0.037733472883701324
Batch: 4. Loss: 0.03983865678310394
Batch: 5. Loss: 0.046939440071582794
Batch: 6. Loss: 0.03857511281967163
Batch: 7. Loss: 0.03522845357656479
Batch: 8. Loss: 0.036839939653873444
Batch: 9. Loss: 0.03970159590244293
Batch: 10. Loss: 0.04768664017319679
Batch: 11. Loss: 0.036082249134778976
Batch: 12. Loss: 0.035102419555187225
Batch: 13. Loss: 0.03426489606499672
Batch: 14. Loss: 0.042694855481386185
Batch: 15. Loss: 0.04000154137611389
-----------------------------------------
Epoch completed in 906.2569191455841 s.
Mean loss: 0.03928626328706741

Training Finished. Saving test images to: ./runs/1536425845.796704
